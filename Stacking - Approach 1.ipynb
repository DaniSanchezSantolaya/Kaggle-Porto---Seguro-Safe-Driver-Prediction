{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach retrains the models with the full train data once we do the k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold_seed = 0\n",
    "models_seed = 0\n",
    "np_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gini metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "    \n",
    "    \n",
    "# Funcitons from olivier's kernel\n",
    "# https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv('data/train.csv', na_values=\"-1\") \n",
    "test_df = pd.read_csv('data/test.csv', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nulls count, not tested\n",
    "train_df['nulls_count'] = train_df.isnull().sum(axis=1)\n",
    "test_df['nulls_count'] = test_df.isnull().sum(axis=1)\n",
    "\n",
    "# from olivier\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "    #\"nulls_count\" #            : not analized\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "    ('ps_ind_02_cat', 'ps_ind_03')\n",
    "    #ps_ind_05_cat|ps_ind_16_bin\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sum_ps_ind_03_ps_calc_02\t0.2891251923\tVALIDATOIN\n",
    "#diff_ps_ind_03_ps_ind_15\t0.289420424\tVALIDATOIN SCORE IMPROVED!!\n",
    "train_df['sum_ps_ind_03_ps_calc_02'] = (train_df['ps_ind_03'] + train_df['ps_calc_02']).astype(np.float32)\n",
    "test_df['sum_ps_ind_03_ps_calc_02'] = (test_df['ps_ind_03'] + test_df['ps_calc_02']).astype(np.float32)\n",
    "train_features.append('sum_ps_ind_03_ps_calc_02')\n",
    "train_df['diff_ps_ind_03_ps_ind_15'] = (train_df['ps_ind_03'] + train_df['ps_ind_15']).astype(np.float32)\n",
    "test_df['diff_ps_ind_03_ps_ind_15'] = (test_df['ps_ind_03'] + test_df['ps_ind_15']).astype(np.float32)\n",
    "train_features.append('diff_ps_ind_03_ps_ind_15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb1_params = {}\n",
    "xgb1_params['objective'] = \"binary:logistic\"\n",
    "xgb1_params['n_estimators'] = 400 # MAX_ROUNDS\n",
    "xgb1_params['learning_rate'] = 0.07\n",
    "xgb1_params['max_depth'] = 4\n",
    "xgb1_params['subsample'] = 0.80\n",
    "xgb1_params['colsample_bytree'] = 0.80\n",
    "xgb1_params['min_child_weight'] = 6\n",
    "xgb1_params['gamma'] = 10\n",
    "xgb1_params['reg_alpha'] = 8\n",
    "xgb1_params['reg_lambda'] = 1.5\n",
    "xgb1_params['scale_pos_weight'] = 1.6\n",
    "#xgb1_params['max_delta_step'] = 0\n",
    "xgb1_params['seed'] = models_seed\n",
    "xgb1 = XGBClassifier(**xgb1_params)\n",
    "\n",
    "xgb2_params = {'eta': 0.025, 'max_depth': 4, \n",
    "          'subsample': 0.9, 'colsample_bytree': 0.7, \n",
    "          'colsample_bylevel':0.7,\n",
    "            'min_child_weight':100,\n",
    "            'alpha':4,\n",
    "            'objective': 'binary:logistic', 'eval_metric': 'auc', 'seed': 99, 'silent': True}\n",
    "\n",
    "# LightGBM\n",
    "lgb1_params = {}\n",
    "lgb1_params['learning_rate'] = 0.02\n",
    "lgb1_params['n_estimators'] = 650\n",
    "lgb1_params['max_bin'] = 10\n",
    "lgb1_params['subsample'] = 0.8\n",
    "lgb1_params['subsample_freq'] = 10\n",
    "lgb1_params['colsample_bytree'] = 0.8   \n",
    "lgb1_params['min_child_samples'] = 500\n",
    "lgb1_params['seed'] = models_seed\n",
    "lgb1 = LGBMClassifier(**lgb1_params)\n",
    "\n",
    "lgb2_params = {}\n",
    "lgb2_params['n_estimators'] = 1090\n",
    "lgb2_params['learning_rate'] = 0.02\n",
    "lgb2_params['colsample_bytree'] = 0.3   \n",
    "lgb2_params['subsample'] = 0.7\n",
    "lgb2_params['subsample_freq'] = 2\n",
    "lgb2_params['num_leaves'] = 16\n",
    "lgb2_params['seed'] = 99\n",
    "lgb2 = LGBMClassifier(**lgb2_params)\n",
    "\n",
    "\n",
    "# Put all models in list\n",
    "models = [xgb1, lgb1, lgb2]\n",
    "models_params = [xgb1_params, lgb1_params, lgb2_params]\n",
    "models_names = ['xgb1', 'lgb1', 'lgb2']\n",
    "model_weights = [0.4, 0.3, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create data set train_meta  and test_meta\n",
    "train_meta = train_df[['id', 'target']].copy()\n",
    "test_meta = pd.DataFrame()\n",
    "test_meta['id'] = id_test\n",
    "for m in models_names:\n",
    "    train_meta[m] = 0\n",
    "    test_meta[m] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Train model xgb1\n",
      "TrainGini =  0.340995849324\n",
      "Val Gini =  0.282453654989\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1\n",
      "TrainGini =  0.398282048294\n",
      "Val Gini =  0.285986558143\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2\n",
      "TrainGini =  0.377447890178\n",
      "Val Gini =  0.281336782324\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.372241929266\n",
      "Mean val error: 0.283258998485\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Fold  1\n",
      "Train model xgb1\n",
      "TrainGini =  0.343265021256\n",
      "Val Gini =  0.285166507865\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1\n",
      "TrainGini =  0.4031174467\n",
      "Val Gini =  0.282233001495\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2\n",
      "TrainGini =  0.380758983816\n",
      "Val Gini =  0.287919904935\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.375713817258\n",
      "Mean val error: 0.285106471432\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Fold  2\n",
      "Train model xgb1\n",
      "TrainGini =  0.343838942595\n",
      "Val Gini =  0.285146908275\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1\n",
      "TrainGini =  0.399996966349\n",
      "Val Gini =  0.285477465816\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2\n",
      "TrainGini =  0.378521715769\n",
      "Val Gini =  0.286305360882\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.374119208238\n",
      "Mean val error: 0.285643244991\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Fold  3\n",
      "Train model xgb1\n",
      "TrainGini =  0.34025600871\n",
      "Val Gini =  0.301064478908\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1\n",
      "TrainGini =  0.398155265157\n",
      "Val Gini =  0.302023240444\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2\n",
      "TrainGini =  0.376412582996\n",
      "Val Gini =  0.302848570534\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.371607952288\n",
      "Mean val error: 0.301978763296\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Fold  4\n",
      "Train model xgb1\n",
      "TrainGini =  0.345620957337\n",
      "Val Gini =  0.27877356498\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1\n",
      "TrainGini =  0.400195476992\n",
      "Val Gini =  0.278704640444\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2\n",
      "TrainGini =  0.381563247066\n",
      "Val Gini =  0.27573971169\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.375793227131\n",
      "Mean val error: 0.277739305704\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Gini for full training set:\n",
      "0.288786258425\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = 0*y\n",
    "\n",
    "\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = kfold_seed, shuffle = True)\n",
    "# Also try with stratified\n",
    "np.random.seed(np_seed)\n",
    "\n",
    "# Run CV\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    # Enocode data\n",
    "    for f in f_cats:\n",
    "        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "        \n",
    "    l_gini_train = []\n",
    "    l_gini_val = []\n",
    "    for model,model_params,model_name, model_weight in zip(models, models_params, models_names, model_weights):\n",
    "        #Fit the base model to the training fold \n",
    "        \n",
    "        print('Train model ' + model_name)\n",
    "        #print('Parameters: ')\n",
    "        #print(model_params)\n",
    "        fit_model = model.fit(X_train, y_train)\n",
    "        # if xgboost model, save it\n",
    "        if 'xgb' in model_name:\n",
    "            fit_model.booster().dump_model(model_name + '_fold' + str(i) + '.dump',  with_stats=True)\n",
    "        \n",
    "        # Train error\n",
    "        pred = fit_model.predict_proba(X_train)[:,1]\n",
    "        l_gini_train.append(eval_gini(y_train, pred))\n",
    "        print(\"TrainGini = \", str(l_gini_train[-1]))\n",
    "        \n",
    "        # make predictions on the test fold. Store these predictions in train_meta to be used as features for the stacking model\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "        l_gini_val.append(eval_gini(y_valid, pred))\n",
    "        print(\"Val Gini = \", str(l_gini_val[-1]))\n",
    "        train_meta.loc[test_index, model_name] = pred\n",
    "              \n",
    "        y_valid_pred.iloc[test_index] += pred * model_weight\n",
    "\n",
    "        print('---------NEXT MODEL---------')\n",
    "    print('Mean train error: ' + str(np.mean(l_gini_train)))\n",
    "    print('Mean val error: ' + str(np.mean(l_gini_val)))\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "        \n",
    "    print('---------------------------------------END FOLD------------------------------------------')\n",
    "    \n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "print(eval_gini(y, y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Enocode data\n",
    "for f in f_cats:\n",
    "    X[f + \"_avg\"], valid, test_df[f + \"_avg\"] = target_encode(\n",
    "        trn_series=X[f],\n",
    "        val_series=X[f].iloc[0:5].copy(),\n",
    "        tst_series=test_df[f],\n",
    "        target=y,\n",
    "        min_samples_leaf=200,\n",
    "        smoothing=10,\n",
    "        noise_level=0\n",
    "        )\n",
    "del valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model xgb1 on full data\n",
      "Train model lgb1 on full data\n",
      "Train model lgb2 on full data\n"
     ]
    }
   ],
   "source": [
    "# Fit each base model to the full training dataset\n",
    "\n",
    "for model,model_name in zip(models, models_names):\n",
    "    print('Train model ' + model_name + ' on full data')\n",
    "    fit_model = model.fit(X, y, verbose=True,)\n",
    "    # make predictions on the test dataset.\n",
    "    pred = fit_model.predict_proba(test_df)[:,1]\n",
    "    # Store these predictions inside test_meta\n",
    "    test_meta[model_name] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Gini =  0.28494696258\n",
      "Val Gini =  0.287284671488\n",
      "Val Gini =  0.287554701478\n",
      "Val Gini =  0.303945679265\n",
      "Val Gini =  0.280061312483\n",
      "0.288758665459\n"
     ]
    }
   ],
   "source": [
    "# K-fold CV for hyperparemeter tuning of the stacked model\n",
    "stacking_model = LogisticRegression(C=0.001)\n",
    "val_gini_stack = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = train_meta.loc[train_index,models_names].copy(), train_meta.loc[test_index,models_names].copy()\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "    pred = stacking_model.predict_proba(X_valid)[:,1]\n",
    "    val_gini_stack.append(eval_gini(y_valid, pred))\n",
    "    print(\"Val Gini = \", str(val_gini_stack[-1]))\n",
    "print(np.mean(val_gini_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.38254782  1.60340036  1.59509317]]\n",
      "[-3.51913946]\n"
     ]
    }
   ],
   "source": [
    "# Fit a new model, S (i.e the stacking model) to train_meta, using predictions of other models as features. \n",
    "# Optionally, include other features from the original training dataset or engineered features\n",
    "stacking_model = LogisticRegression(C=0.01)\n",
    "stacking_model.fit(train_meta[models_names], y)\n",
    "# Use the stacked model S to make final predictions on test_meta\n",
    "res = stacking_model.predict_proba(test_meta[models_names])[:,1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_meta.id\n",
    "sub['target'] = res\n",
    "sub.to_csv('stacked_1.csv', index=False)\n",
    "print(stacking_model.coef_)\n",
    "print(stacking_model.intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show correlation between the different model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x30211da0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAFoCAYAAABDrhLLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xm8XHV9//HXhJawFLQQCxGQH/DTj4hU5CebC4JtcYHW\nlipiUbBQZTFlEdu0tYgptQiy/hAEoVhwqaGihQIiZSkYjApC2Pzxia0BS4ipCWWRENnu749zbhkm\nJ2Rm7ty593t5PXnM49455zvnfpJ5hPf9Luc7rZGRESRJUrmmTXQBkiRpbAxzSZIKZ5hLklQ4w1yS\npMIZ5pIkFc4wlySpcIa5JEmFM8wlSSqcYS5JUuF+ZaILkCRpMouI6cCtwEcz86bVtHk98Hlge+Bu\n4PDMvK3t/PuBE4CZwLeBD2fm8rbznwEOpupk/31mzu6lRnvmkiStRh3k/wi85gXarAdcCdwI7AjM\nB66MiHXr8zsDFwDHA7sAvw78Q9vrjwX2B94N/CFwQER8rJc6DXNJkhpExLbA94Ct1tB0f2BFZs7O\nytHAY8B76/MfBeZm5lcy827gg8C7ImLL+vyRwHGZOT8zbwRmA7N6qdUwlySp2VuB64DdgNYLtNsF\nmNdx7Ob6dQC7Av8zPJ+ZDwA/BXaNiJnAFsB32l47D9gyIjbptlDnzCVJapCZ545+HxEv1HQm1Tx5\nu6XAdm3nH2w4v3l9bqTj/FKqXx42r79fI3vmkiSNzXrALzuO/RKY3sX59QAy88mOc7S9fo0mTc/8\nN7d8qx+s/iLxg9vmTnQJGqJfLFo00SVoyGa8YbcXGpIek7FkxZ333zheda1k1eCdDqzo4vxKgIhY\nuy3QR9uuoEv2zCVJGpvFwKYdxzYFlnRxfjHVkPqmHedG2l6/Roa5JKkYrVar78c4+h7wxo5jb6K6\nRW30/JtHT0TEFlTz4fMzcwnVYrg3t732LcBPM7Or+XKYRMPskiStSas1Ofqg9UrzRzJzJfB14MSI\nOB34AnAY1Vz4P9XNPw/cEBHfo9p85gzgXzLzp23nT4qI0V76icBne6lncvytSJI0uXXO1S8B9gPI\nzMeAfYDdqcJ6Z+CdmflEff57wKFUm8bMA5ZT7fY26rPAXOAb9deLMvPMXoprjYxMjnVnLoB78XAB\n3IuLC+BefMZzAdwOW72t76xYsOj6cR1rn0gOs0uSijHtBfduefEyzCVJxRjnhWzFMswlScWYNkkW\nwE02hrkkqRj2zJv5K44kSYUzzCVJKpzD7JKkYrRczd7IMJckFcMFcM0Mc0lSMVwA18wwlyQVY5ph\n3sjxCkmSCmeYS5JUOIfZJUnFaNkHbWSYS5KK4QK4Zoa5JKkYLoBrZphLkorhpjHNnHyQJKlwhrkk\nSYVzmF2SVAy3c21mmEuSiuFq9maGuSSpGK5mb2aYS5KK4Wr2Zk4+SJJUOMNckqTCOcwuSSqGq9mb\nGeaSpGK4mr2ZYS5JKoar2ZsZ5pKkYriavZmTD5IkFc4wlySpcA6zS5KK4QK4Zoa5JKkYLoBrZphL\nkorhArhmzplLklQ4e+aSpGK4A1wz/1YkSSpcVz3ziLgBGOmmbWa+bUwVSZK0Gq5mb9btMPsFwHnA\nfwDfGL9yJElaPVezN+sqzDPzKxHxIHAVcF1mzhvfsiRJWpWr2Zt1PWeemTcAnwH+bvzKkSRJvepp\nNXtmzgHmjFMtkiS9IIfZm7maXZKkwvV1n3lE7A4cBmwLPAncC5yemQsGWJskSc/javZmPffMI2IW\ncA2wEvh74CvAWsD3ImL/wZYnSdJzprVafT+msn565rOBj2Tmxe0HI+I7VIvjvjaIwiRJ6uRq9mb9\nzJlvCNzScPw7wMvGVo4kSepVP2H+OeCkiHjp6IGIWAc4Hjh/UIVJktTJYfZm3W7nuojntnNtAVsC\niyPiJ8AzwDbAusDt41GkJElavW7nzD81nkVIktQNV7M363Y714vGuxBJktZkqg+X96vn1ewdQ+7t\nRqjuOV8CXJKZ546xNkmSnsfV7M36XQA3g+oWtKOAo4EvAxsD36L6MJa/jog/H1SRkiRp9fq5z/xA\nqvvM2+8nvzwi7gQ+kZmvj4gFVCvbTx5EkZIkgcPsq9NPz/x/A03btt4NvLr+fiGwSb9FSZKk7vUT\n5vOBORGx/uiB+vvjeW4zmXcBPx57eZIkPafVavX9mMr6GWb/MHAl8GBELKS67/yVwAPAH0TEXsAZ\nwHsHVqUkSTjMvjr9hPlrge2B36q/Pg3cA9wMfIIqyLfIzJ8PqkhJkrR6/YT5JcDXgaMy8xqAiNiD\nah59/cz868GVJ0nSc6b6cHm/+pkzfz3Vdq73RMQfRcTfA98G/oXnFsBJkjRwrTH8N5X13DPPzHuB\n3SPiy1T3lz8N7DPaS5ckScPVzw5wWwOnA79NtWf7DsClEXECcFpmPj3QCiVJqk2b2h3svvUzZ/4j\n4LvADpn5Y4CIeDdwFvDHwLaDK0+SpOcMc848IqYD5wD7AiuAUzPztNW03Ytqo7RtqG7hnpWZC9vO\nfxw4Angp8E3gyMx8vD73EuBUYB+q6e8rgaMz85Fua+1nzvyIzHzbaJADZOZlwHbAv/ZxPUmSJqNT\ngB2BPaiC+PiI2LezUURsB1xBFdI7Un0c+PURsV59/lDgk8BfAG8CNge+2naJ86juDnsHsBdVp/gL\nvRTaz5z5has5/hhwZK/XkySpW8O6z7wO4kOAt2fmHcAdEXEyMAv4Rkfzw4CbM3NO/Xx2ROwDHEC1\ntfks4JTMvKS+9kHA4oh4JbCYquf/xsxcUJ8/GrgpItbOzCe7qbefnrkkSRNiiDvAvY6qwzu/7dg8\nYJeGtlsD3+84dhewW9v5H4yeyMyfAT+vzz9LNbx+R/sfE1gL+LVui+1nzlySpKluJrCsY1H3UmCd\niNg4M5d3HN+s4/VbAMubztdboG8EzMjMlUDn3WBHAXdm5kPdFmuYS5KKMW1494uvB/yy49jo8+kd\nx+cCl0XE14CrgQ8AOwHXt53/y4i4GbgPOA0YAdbu/KERMQt4D/D2Xop1mF2SVIwhDrOvZNXQHn2+\nov1gZn4bmANcWr/uAOAi4NG6yQnArVRbn/838ATVrqmPtl8nIo4AzqRayX5dL8Ua5pIkrWoxMCMi\n2nNyU+CJzHy4s3FmnghsAMzMzL2ADal64WTmiszcn+q2tJdl5tHA/xo9D/9z69rngI9n5ud6LdZh\ndklSMYb4qWkLgKeAXan2VgF4C8991Pf/iIj9gV0y8xhgWUSsC+wJHFifPwm4JzMvrp/vRBX2362f\nHwScRPWZJ2f1U6xhLkkqxrCyPDOfiIiLgXMj4mCqe8OPBQ4CiIhNgEfqBWwLgQsj4ibgbqrNY+7P\nzKvryz0IfDIifkQ1V/4l4JzMfDgifp1q07WLgEvq6476eWY+2029DrNLktTsY8APqRaynQUcV2+S\nBrAE2A8gM28DDqfaxe0W4Bmq281GnQVcDnyLane3y4E/q8/tBaxP9UvCg/VjSf11824LbY2MjPT8\npxsPv7nlWydHIRp3P7ht7kSXoCH6xaJFE12ChmzGG3Ybt/7zX+w1u++s+Mw1J03Znd0dZpckFWOq\nf5RpvxxmlySpcPbMJUnFGOanppXEMJckFWOIt6YVxTCXJBXDLG/mnLkkSYUzzCVJKpzD7JKkYjhn\n3swwlyQVw/vMmxnmkqRi2DNvZphLkophljdzAZwkSYUzzCVJKpzD7JKkYridazPDXJJUDBfANTPM\nJUnFMMubGeaSpGLYM2/mAjhJkgpnmEuSVDiH2SVJxXA712aGuSSpGN6a1swwlyQVY5pZ3sgwlyQV\nw555MxfASZJUOMNckqTCTZph9h/cNneiS9CQ7Lzj+ya6BA3RrXd9Y6JL0BTiMHuzSRPmkiStiQvg\nmhnmkqRi2DNvZphLkophljdzAZwkSYUzzCVJKpzD7JKkYvgRqM0Mc0lSMfyglWaGuSSpGHbMmxnm\nkqRiOMzezAVwkiQVzjCXJKlwDrNLkorhDnDNDHNJUjHM8maGuSSpGPbMmxnmkqRi+KlpzVwAJ0lS\n4QxzSZIK5zC7JKkYzpk3M8wlScUwy5sZ5pKkYridazPnzCVJKpw9c0lSMZwzb2bPXJKkwtkzlyQV\nw455M8NcklQMh9mbGeaSpGKY5c2cM5ckqXD2zCVJxfA+82b2zCVJKpw9c0lSMeyYNzPMJUnFcDV7\nM8NcklQMs7yZc+aSJBXOnrkkqRgOszczzCVJahAR04FzgH2BFcCpmXnaatruBZwMbAPMB2Zl5sK2\n858CDgHWB66pzy9ruM7ZwGsyc89eanWYXZJUjFar/0cfTgF2BPYAjgCOj4h9OxtFxHbAFcA36/a3\nA9dHxHr1+UOBPwbeD7wZeDlwfsN13ggcBoz0Wqg9c0lSMYa1aUwdxIcAb8/MO4A7IuJkYBbwjY7m\nhwE3Z+ac+vnsiNgHOIAqtN8JzM3MefW1Twa+2vHzfhU4D/huP/XaM5ckFWOIPfPXUXV457cdmwfs\n0tB2a+D7HcfuAnarv18O7B0RL4+IdYE/Am7raP+XwB3AtT1XimEuSVKTmcCyzHy67dhSYJ2I2Lij\n7VJgs45jWwAz6u//BngGeAB4FHgTVaADEBGvpurdH9NvsYa5JKkYrVar70eP1gN+2XFs9Pn0juNz\ngfdGxN4RsVZEHATsBKxdn98KeBzYG9idKtS/2Pb684BPZubPey1ylHPmkiStaiWrhvbo8xXtBzPz\n2xExB7gUWAu4AbgIeEnd5CLg2Mz8FkBEvA+4PyJ2olowNy0zLxhLsfbMJUnFGOKc+WJgRkS05+Sm\nwBOZ+XBn48w8EdgAmJmZewEbAvdFxMuohtzvbGv7ALAM2BJ4H/CGiHgsIh4D/grYPSIejYjNuy22\n6555RPwH0NVfR2Zu3e11JUnq1hA3jVkAPAXsynMrzN8C3NLZMCL2B3bJzGOAZfUitz2BA4GHqIbn\nXwMsrNvPADYGFlGteF+37XJHATtTzak/2G2xvQyzHwT8E/Az4IweXidJUlEy84mIuBg4NyIOBjYH\njqXKQiJiE+CRzFxJFdIXRsRNwN1Um8fcn5lX122/CJwSEcuB/wY+C3w3M3/Y+XMj4iGq3v+iXurt\nOswzc169w83NwMOZeVkvP0iSpLEa8m6uH6PaAe564BHguLbsWwJ8CLg4M2+LiMOBU4GNqG4v26ft\nOkcDfwt8haoXfg3wgUEW2hoZ6W2jmXonm7dn5iq74IzFyuU/63nHG5Vp5x3fN9ElaIhuvatzfw1N\ndWtvuPG4Re6VHzu776zY+7SPTtmN3XtezZ6Z51Eto5ckSZOAt6ZJkorhh6Y16zvMI+JAqh1rtgWe\nBO4FTnMuXZI0XvwI1GZ93WceESdQrWi/Gvgg8GGqPWu/FBFHD648SZK0Jv32zA8FDszMK9qOXR4R\nC4Az8dY1SdI4sGPerN8wbwE/bTi+kOff/C5J0sAM6yNQS9Pvdq5zgPPqT3oBICK2oOqRf3oQhUmS\n1GmI27kWpZftXJ8F2u/vawH3RMTjwLNUe9KOANsBpwyySEmStHq9DLPvOW5VSJLUBVezN+tlO9cb\nx7MQSZLUn74WwDUMubd7kmrP2kuo9rF9qs/aJEl6HjvmzfpdAHc4sLT+ugPweqp7zR+k+rSYPwf2\nptpYXpKkgWhNa/X9mMr6vTXtz4CDRz/erXZnRPwncHZmHh8Ri4FLgdljLVKSJLBnvjr99sw3AR5o\nOP4zYLP6+yXAhn1eX5IkdanfMP9X4OyI2HL0QP39mcB1EbEWcDBw19hLlCSp0mq1+n5MZf0Os/8J\nMBdYFBHLqe4534hqr/aPAO+imk9/9yCKlCRJq9fLpjGv6Dh0CDAd+D3gKeAqqpXsawPXAr+RmX1/\niLwkSZ2meAe7b730zO+j+Xa00b/a0+vvRzJzrTHWJUnSKqb6cHm/egnzrcatCkmSumCWN+tlB7j7\nx7MQSZLUn35Xs0uSpEmi39XskiQNn+PsjQxzSVIxXADXzDCXJBXDLG9mmEuSijHVPzClXy6AkySp\ncIa5JEmFc5hdklQM58ybGeaSpGK4mr2ZYS5JKoZZ3swwlyQVw555MxfASZJUOMNckqTCOcwuSSqG\no+zNDHNJUjGcM29mmEuSyuHkcCPDXJJUDHvmzfwdR5KkwhnmkiQVzmF2SVIxHGVvZphLkorhnHkz\nw1ySVAyzvJlhLkkqh2neyAVwkiQVzjCXJKlwDrNLkorRmuYwexPDXJJUDKfMmxnmkqRieGtaM8Nc\nklQMs7yZC+AkSSqcYS5JUuEcZpcklcNx9kaGuSSpGN6a1swwlyQVw455M8NcklQO07yRC+AkSSrc\npOmZ/2LRookuQUNy613fmOgSNERv2H7fiS5BQ3bn/TdOdAkvOpMmzCVJWhNH2ZsZ5pKkYriavZlh\nLkkqhnuzNzPMJUnlMMsbGeaSJDWIiOnAOcC+wArg1Mw8bTVt9wJOBrYB5gOzMnNhfe5ZYIRVfxU5\nMDO/XLeZAxxKlcuXAn+amU92W6u3pkmS1OwUYEdgD+AI4PiIWOX2jIjYDrgC+Gbd/nbg+ohYr26y\nKTCz/ropVejfB1xWv/4vgMOA9wHvAN4GHN9LofbMJUnFGNaceR3EhwBvz8w7gDsi4mRgFtB5f+1h\nwM2ZOad+Pjsi9gEOAM7PzP9qu+5WwJHA3pn5WERMA44Bjs3MG+s2nwQO6qVee+aSpGK0Wq2+Hz16\nHVWHd37bsXnALg1ttwa+33HsLmC3hrZ/A1ybmTfUz7cDNqbupQNk5j9m5jt6KdaeuSSpHMPrgs4E\nlmXm023HlgLrRMTGmbm84/hmHa/fAmhvQ0S8Ang/sGvb4a2Bh4A3RcTfATOo5sxn9zJnbphLkrSq\n9YBfdhwbfT694/hc4LKI+BpwNfABYCfg+o52hwC3ZOatbcd+DVgfOBE4miqXz6P6teWobot1mF2S\nVIwhDrOvZNXQHn2+ov1gZn4bmEPVo15JNVd+EfBox+v/EPhyx7GngXWoVq/fmJnXAccCf9JLsYa5\nJEmrWgzMqBeojdoUeCIzH+5snJknAhsAMzNzL2BDqhXrAETE5sC2tM2N15aMXqL9clTD+S/rtliH\n2SVJxRjiDnALgKeo5re/Wx97C3BLZ8OI2B/YJTOPAZZFxLrAnjx/RfouwH9m5gMdL78deJJqwd21\n9bHXAI/RMef+QgxzSVI5hpTlmflERFwMnBsRBwObUw1/HwQQEZsAj2TmSmAhcGFE3ATcTXUf+f2Z\n+a22S74W+FHDz3ksIi4AzoqID1GNmH+G6pa2Z7ut12F2SVIxWtNafT/68DHgh1QL2c4CjsvM0WHy\nJcB+AJl5G3A4cCpVz/0ZYJ+Oa20C/Pdqfs4xwLeAq6g2n7kK+KteCm2NjIz00n7cLLt1/uQoRONu\nw1e9aqJL0BD5eeYvPnfef+O49Z9/csk/950VW+/3+1N2Z3eH2SVJ5fBT0xo5zC5JUuHsmUuSimHH\nvJlhLkkqxhBvTSuKYS5JKkd/q9KnPOfMJUkqnD1zSVIxHGZvZs9ckqTC2TOXJJXDjnkjw1ySVAyH\n2ZsZ5pKkYvS5x/qU55y5JEmFs2cuSSqHw+yNDHNJUjGcM2/mMLskSYWzZy5JKocd80b2zCVJKpw9\nc0lSMbw1rZlhLkkqhwvgGhnmkqRiuJq9mXPmkiQVzp65JKkczpk3smcuSVLh7JlLkorhnHkzw1yS\nVA6zvFFPYR4R2wP7AS8Brs3MyzvObwickZkHD65ESZIq9sybdT1nHhG/C9wK7AS8Crg0Iq6PiI3b\nmq0LHDTYEiVJ0gvpZQHcCcAxmfmOzHwHsAPwcmBeRGwyLtVJktRuWqv/xxTWS5hvA1w9+iQz7wHe\nDDwF3BARLxtwbZIkqQu9hPm/A+9sP5CZy4DfAdYCbgC2GFxpkiQ9X6vV6vsxlfUS5n8NnBoRV9YL\n4QDIzKXA24AR4N8GW54kSW1arf4fU1jXYZ6ZVwI7A3cCT3acWwzsApwJLBxkgZIkjbJn3qynW9My\n806qMG86twL4RP2QJElD0vemMRFxIHAYsC1VT/1e4LTMvGxAtUmS9HxTfFV6v/ramz0iTgDOoFrd\n/kHgw8A84EsRcfTgypMkSWvSb8/8UODAzLyi7djlEbGAat78jDFXJklSh6k+992vfsO8Bfy04fhC\nql3gJEkaPMO8Ub8fgToHOC8iXj16ICK2oOqRf3oQhUmS1Kk1rdX3YyrrumceEc9S3Us+qgXcExGP\nA88CG9TntwNOGWSRkiRp9XoZZt9z3KqQJEl96zrMM/PG8SxEkqQ1cs68UV8L4BqG3Ns9CSwBLgGO\ny8yn+qxNkqTncTV7s34XwB0OLK2/7gC8nupe8weBk4E/B/YG/nYANUqSVHFv9kb93pr2Z8DBmXl1\n27E7I+I/gbMz8/iIWAxcCswea5GSJAFTflV6v/rtmW8CPNBw/GfAZvX3S4AN+7y+JEnqUr9h/q/A\n2RGx5eiB+vszgesiYi3gYOCusZcoSZJeSL/D7H8CzAUWRcRyqnvON6Laq/0jwLuo5tPfPYgiJUkC\npvzcd7962TTmFR2HDgGmA78HPAVcRbWSfW3gWuA3MnN1K94lSeqdYd6ol575fTTfjjb6N3t6/f1I\nZq41xrokSVqFt6Y16yXMtxq3KiRJ6oar2Rv1sgPc/eNZiCRJ6k+/q9klSdIk0e9qdkmShq7Vsg/a\nxDCXJJXDBXCNDHNJUjFczd7MMJcklcPV7I2cfJAkqXCGuSRJhXOYXZJUDOfMmxnmkqRyGOaNDHNJ\nUjm8z7yRYS5JKkZriKvZI2I6cA6wL7ACODUzT1tN272Ak4FtgPnArMxc2Hb+PcCngc2AecBHMvOn\n9bmXAp8D3ln/nC9l5l/1Uqu/4kiS1OwUYEdgD+AI4PiI2LezUURsB1wBfLNufztwfUSsV59/I/BV\n4LPA66k+LvxrbZf4PLAp8CbgA8CHIuKoXgo1zCVJ6lAH8SHAkZl5R2ZeRtXzntXQ/DDg5syck5k/\nzszZwCPAAfX5Y4GLM/OCzPwxcCSwaURsVJ9/J3BaZt6bmTdSBf9v9VKvw+ySpHIMbwHc66gycn7b\nsXlA0/D31sD3O47dBewGnE/Vsz9w9ERm3le/ZtRy4AMRcQPw68A7gK/3Uqw9c0lSMVqtVt+PHs0E\nlmXm023HlgLrRMTGHW2XUs2Ft9sCmBERL6EK6F+NiKsjYklE/HNEvLyt7RHAbwOPAQ8Ai4G/6aVY\nw1ySVI7WtP4fvVkP+GXHsdHn0zuOzwXeGxF7R8RaEXEQsBOwNvBrdZszgYuBferXX9H2+lcDt1D1\n5P8AeC0wu5diHWaXJBVjiKvZV7JqaI8+X9F+MDO/HRFzgEuBtYAbgIuAlwCjPfvzM/OrABFxALA0\nInYFllEttNssM/+rPr8+cE5EnJSZz3ZTrD1zSZJWtZhqmLw9JzcFnsjMhzsbZ+aJwAbAzMzcC9gQ\nuI8qrJ8Csq3tQ1Tz5FtQrW7/+WiQ126vr7URXTLMJUla1QKqEN617dhbqIbDnyci9o+I0zPzqcxc\nFhHrAnsC12fmM8APqRbUjbafAcygCvsHqX5pmNF2yW2BX2Tmsm6LdZhdklSOIa1mz8wnIuJi4NyI\nOBjYnOoWs4MAImIT4JHMXAksBC6MiJuAu6luYbs/M6+uL3cq8MWIWADcU5+/LTNviYi1gB8BF0fE\nx4GX1efP6qVee+aSpGIMcTU7wMeoetXXU4XrcfX95gBLgP0AMvM24HCq0L4FeIZqoRv1+UuBY6g2\njRnt2f9+fe4Z4F3A48BNVHPtXwWO76XQ1sjISM9/uvGw7Nb5k6MQjbsNX/WqiS5BQ/SG7VfZMEtT\n3J333zhu3ecVS+7vOyvWm7nllP2UFofZJUnlGOLe7CVxmF2SpMIZ5pIkFc5hdklSMfpcyDblGeaS\npHL0vi3ri4JhLkkqhj3zZv6KI0lS4eyZS5LK4TB7I/9WJEkqnD1zSVIxhvgRqEUxzCVJ5XABXCPD\nXJJUjJZz5o38W5EkqXCT5lPTJElSf+yZS5JUOMNckqTCGeaSJBXOMJckqXCGuSRJhTPMJUkqnGEu\nSVLhDHNJkgpnmEuSVDjDXJKkwhnmEygiDoqIRV22PSAibhjvmtS9iNgyIp6NiFesod1bI+LZLq/5\n5oj4j8FUqEEa5PsdEXtHxO0R8VhELIiI3x1stXqxMcwn3ho3x4+IPYHzummroev2Penmfd4e+CfA\nz3icvMb8fkfEbwKXAhcArwO+AHy9fv+lvhjmk1xEHA9cBdhbm8Ii4lDgZuBnE12Lxt37gesy8+zM\n/ElmngPcAOw3wXWpYH6e+QBExMHAOcBrMvMnEfFq4Haqf5x3A+cDuwH/DlwMzMrMreqXtyLi08As\n4FHgpMz8XNvlfwv4nfrrW4fx51HvImIjqp7W7wBLgc8Cn8/MaW1tZgHHU/XazsvM49ou8Xbgg8BL\n6zaaxMb4fv8DsHbDZV8ynjVrarNnPgCZeSHwXeD0+tAXgK9T9aivBJYD/wc4kef+cY/aEtge2BX4\nBHBKROzedu3dM3PeeP8ZNGZzgY2pfmlr/5/4qBZwANUvZQcDH42IA0dPZua+mXnZ8MrVGPX9fmfl\nrtGGEbFd3e7a4ZSuqcie+eAcCiyIiC8DrwR+n+of6GbATpn5OHBvPV+2f9vrngAOzMyHgf8XEXsA\nhwE3DbN4jcl0qvd6q8y8H7g7Ij4FfL6tzQjwx5l5L3BnRJxB9T5fPOxiNWYDe78jYgbV/Pl3MvPy\nYRSvqcme+YBk5o+Bk4A/Ao7NzIeoetwL6yAfNb/jpT+pg3zUbcC241qsBqkF/B6wvP4f+6jO9/nx\n+n/so3yfyzSw9zsiNgGupwr+945DrXoRMcwHawfgaarf2qm/71yZ3Pn8mY7n04AnB1+axtFTrPl9\n7rxVyfe5XGN+vyNiM6rRt18B9sjM5YMuUi8uhvmARMS7qRbD7AN8oB4uvwd4ZUSs39b0DR0v3SYi\n1ml7vjNwLyrFCNXaiI0iYsu2453v8wYRsUXb813wfS7RmN/viFgPuJrql4K3ZubScaxXLxLOmQ9A\nRGwAnAX3KaJgAAABhUlEQVSckJnXRMRZVPeF7wA8AFwQEXOA1wJHUi2IG7UucFF9/i3Ae6gWw6kM\nLaoe19XAFyPiKGBTYE5HuxHg4og4GngV8KdUq9dVlkG8358AtgL2AKbVw+0AT2Tmo+NbvqYqe+aD\n8WngceC0+vmngPWp/tH+AdUiuNvr5xfy/OHV24HFwPeB2cCHMnPBUKrWIIyuYD4Y+AXwPeBsVn2f\nH6K6s+HfgDOBT7p6vUiDeL/3pfol/vvAg22PM8a5dk1hrZERNxUbL/VK1R0z85q2Yx8H3pWZb5u4\nyjRIEbEu8NvAVZn5TH3sPcDJmbn1hBangfP91mTkMPv4agGX10NtV1ENtx0N/O2EVqVBW0nVM/t8\nRFwIzKS67/iSCa1K48X3W5OOw+zjKDN/TnXLyeFUi1/OB/5vZp47oYVpoDJzBBhdAHk31X3DVwHH\nvdDrVCbfb01GDrNLklQ4e+aSJBXOMJckqXCGuSRJhTPMJUkqnGEuSVLhDHNJkgpnmEuSVDjDXJKk\nwv1/Ykn3H8C5iBcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3021ba58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "corr = train_meta[models_names].corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
