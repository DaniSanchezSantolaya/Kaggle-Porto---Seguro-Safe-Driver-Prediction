{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold_seed = 0\n",
    "models_seed = 0\n",
    "np_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gini metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "    \n",
    "    \n",
    "# Funcitons from olivier's kernel\n",
    "# https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "\n",
    "# def target_encode_2(trn_series, val_series, tst_series, target):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv('data/train.csv', na_values=\"-1\") \n",
    "test_df = pd.read_csv('data/test.csv', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nulls count, not tested\n",
    "train_df['nulls_count'] = train_df.isnull().sum(axis=1)\n",
    "test_df['nulls_count'] = test_df.isnull().sum(axis=1)\n",
    "\n",
    "# from olivier\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "    #\"nulls_count\" #            : not analized\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "    ('ps_ind_02_cat', 'ps_ind_03')\n",
    "    #ps_ind_05_cat|ps_ind_16_bin\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sum_ps_ind_03_ps_calc_02\t0.2891251923\tVALIDATOIN\n",
    "#diff_ps_ind_03_ps_ind_15\t0.289420424\tVALIDATOIN SCORE IMPROVED!!\n",
    "train_df['sum_ps_ind_03_ps_calc_02'] = (train_df['ps_ind_03'] + train_df['ps_calc_02']).astype(np.float32)\n",
    "test_df['sum_ps_ind_03_ps_calc_02'] = (test_df['ps_ind_03'] + test_df['ps_calc_02']).astype(np.float32)\n",
    "train_features.append('sum_ps_ind_03_ps_calc_02')\n",
    "train_df['diff_ps_ind_03_ps_ind_15'] = (train_df['ps_ind_03'] + train_df['ps_ind_15']).astype(np.float32)\n",
    "test_df['diff_ps_ind_03_ps_ind_15'] = (test_df['ps_ind_03'] + test_df['ps_ind_15']).astype(np.float32)\n",
    "train_features.append('diff_ps_ind_03_ps_ind_15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]\n",
    "f_bin = [f for f in X.columns if \"_bin\" in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_sets = {}\n",
    "feature_sets['FS1'] = ['ps_car_13', 'ps_reg_03', 'ps_ind_05_cat', 'ps_ind_03', 'ps_ind_15',\n",
    "       'ps_reg_02', 'ps_car_14', 'ps_car_12', 'ps_car_01_cat', 'ps_car_07_cat',\n",
    "       'ps_ind_17_bin', 'ps_car_03_cat', 'ps_reg_01', 'ps_car_15', 'ps_ind_01',\n",
    "       'ps_ind_16_bin', 'ps_ind_07_bin', 'ps_car_06_cat', 'ps_car_04_cat',\n",
    "       'ps_ind_06_bin', 'ps_car_09_cat', 'ps_car_02_cat', 'ps_ind_02_cat',\n",
    "       'ps_car_11', 'ps_car_05_cat', 'ps_calc_09', 'ps_calc_05',\n",
    "       'ps_ind_08_bin', 'ps_car_08_cat', 'ps_ind_09_bin', 'ps_ind_04_cat',\n",
    "       'ps_ind_18_bin', 'ps_ind_12_bin', 'ps_ind_14',\n",
    "       'sum_ps_ind_03_ps_calc_02', 'diff_ps_ind_03_ps_ind_15',\n",
    "       'ps_reg_01_plus_ps_car_02_cat', 'ps_reg_01_plus_ps_car_04_cat',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03', 'ps_ind_05_cat_avg',\n",
    "       'ps_car_01_cat_avg', 'ps_car_07_cat_avg', 'ps_car_03_cat_avg',\n",
    "       'ps_car_06_cat_avg', 'ps_car_04_cat_avg', 'ps_car_09_cat_avg',\n",
    "       'ps_car_02_cat_avg', 'ps_ind_02_cat_avg', 'ps_car_05_cat_avg',\n",
    "       'ps_car_08_cat_avg', 'ps_ind_04_cat_avg',\n",
    "       'ps_reg_01_plus_ps_car_02_cat_avg', 'ps_reg_01_plus_ps_car_04_cat_avg',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03_avg']\n",
    "\n",
    "feature_sets['FS2'] = ['ps_car_13', 'ps_reg_03', 'ps_ind_03', 'ps_ind_15',\n",
    "       'ps_reg_02', 'ps_car_14', 'ps_car_12',\n",
    "       'ps_ind_17_bin', 'ps_car_03_cat', 'ps_reg_01', 'ps_car_15', 'ps_ind_01',\n",
    "       'ps_ind_16_bin', 'ps_ind_07_bin', \n",
    "       'ps_ind_06_bin', \n",
    "       'ps_car_11', 'ps_calc_09', 'ps_calc_05',\n",
    "       'ps_ind_08_bin', 'ps_ind_09_bin',\n",
    "       'ps_ind_18_bin', 'ps_ind_12_bin', 'ps_ind_14',\n",
    "       'sum_ps_ind_03_ps_calc_02', 'diff_ps_ind_03_ps_ind_15',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03', 'ps_ind_05_cat_avg',\n",
    "       'ps_car_01_cat_avg', 'ps_car_07_cat_avg', 'ps_car_03_cat_avg',\n",
    "       'ps_car_06_cat_avg', 'ps_car_04_cat_avg', 'ps_car_09_cat_avg',\n",
    "       'ps_car_02_cat_avg', 'ps_ind_02_cat_avg', 'ps_car_05_cat_avg',\n",
    "       'ps_car_08_cat_avg', 'ps_ind_04_cat_avg',\n",
    "       'ps_reg_01_plus_ps_car_02_cat_avg', 'ps_reg_01_plus_ps_car_04_cat_avg',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03_avg', 'logsum_mean_target_all']\n",
    "\n",
    "# Same than FS2, but without logsum\n",
    "feature_sets['FS3'] = ['ps_car_13', 'ps_reg_03', 'ps_ind_03', 'ps_ind_15',\n",
    "       'ps_reg_02', 'ps_car_14', 'ps_car_12',\n",
    "       'ps_ind_17_bin', 'ps_car_03_cat', 'ps_reg_01', 'ps_car_15', 'ps_ind_01',\n",
    "       'ps_ind_16_bin', 'ps_ind_07_bin', \n",
    "       'ps_ind_06_bin', \n",
    "       'ps_car_11', 'ps_calc_09', 'ps_calc_05',\n",
    "       'ps_ind_08_bin', 'ps_ind_09_bin',\n",
    "       'ps_ind_18_bin', 'ps_ind_12_bin', 'ps_ind_14',\n",
    "       'sum_ps_ind_03_ps_calc_02', 'diff_ps_ind_03_ps_ind_15',\n",
    "       #'ps_ind_02_cat_plus_ps_ind_03', \n",
    "       'ps_ind_05_cat_avg',\n",
    "       'ps_car_01_cat_avg', 'ps_car_07_cat_avg', 'ps_car_03_cat_avg',\n",
    "       'ps_car_06_cat_avg', 'ps_car_04_cat_avg', 'ps_car_09_cat_avg',\n",
    "       'ps_car_02_cat_avg', 'ps_ind_02_cat_avg', 'ps_car_05_cat_avg',\n",
    "       'ps_car_08_cat_avg', 'ps_ind_04_cat_avg',\n",
    "       'ps_reg_01_plus_ps_car_02_cat_avg', 'ps_reg_01_plus_ps_car_04_cat_avg',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03_avg']\n",
    "\n",
    "# Same than FS3, but changing mean target by bayes prob and without logsum\n",
    "feature_sets['FS4'] = ['ps_car_13', 'ps_reg_03', 'ps_ind_03', 'ps_ind_15',\n",
    "       'ps_reg_02', 'ps_car_14', 'ps_car_12',\n",
    "       'ps_ind_17_bin', 'ps_car_03_cat', 'ps_reg_01', 'ps_car_15', 'ps_ind_01',\n",
    "       'ps_ind_16_bin', 'ps_ind_07_bin', \n",
    "       'ps_ind_06_bin', \n",
    "       'ps_car_11', 'ps_calc_09', 'ps_calc_05',\n",
    "       'ps_ind_08_bin', 'ps_ind_09_bin',\n",
    "       'ps_ind_18_bin', 'ps_ind_12_bin', 'ps_ind_14',\n",
    "       'sum_ps_ind_03_ps_calc_02', 'diff_ps_ind_03_ps_ind_15',\n",
    "       #'ps_ind_02_cat_plus_ps_ind_03', \n",
    "       'ps_ind_05_cat_bayes_prob', 'ps_car_01_cat_bayes_prob',\n",
    "       'ps_car_07_cat_bayes_prob', 'ps_car_03_cat_bayes_prob',\n",
    "       'ps_car_06_cat_bayes_prob', 'ps_car_04_cat_bayes_prob',\n",
    "       'ps_car_09_cat_bayes_prob', 'ps_car_02_cat_bayes_prob',\n",
    "       'ps_ind_02_cat_bayes_prob', 'ps_car_05_cat_bayes_prob',\n",
    "       'ps_car_08_cat_bayes_prob', 'ps_ind_04_cat_bayes_prob',\n",
    "       'ps_reg_01_plus_ps_car_02_cat_bayes_prob',\n",
    "       'ps_reg_01_plus_ps_car_04_cat_bayes_prob',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03_bayes_prob']\n",
    "\n",
    "# Same than FS1, Added binaries avg target\n",
    "feature_sets['FS5'] = ['ps_car_13', 'ps_reg_03', 'ps_ind_05_cat', 'ps_ind_03', 'ps_ind_15',\n",
    "       'ps_reg_02', 'ps_car_14', 'ps_car_12', 'ps_car_01_cat', 'ps_car_07_cat',\n",
    "       'ps_ind_17_bin', 'ps_car_03_cat', 'ps_reg_01', 'ps_car_15', 'ps_ind_01',\n",
    "       'ps_ind_16_bin', 'ps_ind_07_bin', 'ps_car_06_cat', 'ps_car_04_cat',\n",
    "       'ps_ind_06_bin', 'ps_car_09_cat', 'ps_car_02_cat', 'ps_ind_02_cat',\n",
    "       'ps_car_11', 'ps_car_05_cat', 'ps_calc_09', 'ps_calc_05',\n",
    "       'ps_ind_08_bin', 'ps_car_08_cat', 'ps_ind_09_bin', 'ps_ind_04_cat',\n",
    "       'ps_ind_18_bin', 'ps_ind_12_bin', 'ps_ind_14',\n",
    "       'sum_ps_ind_03_ps_calc_02', 'diff_ps_ind_03_ps_ind_15',\n",
    "       'ps_reg_01_plus_ps_car_02_cat', 'ps_reg_01_plus_ps_car_04_cat',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03', 'ps_ind_05_cat_avg',\n",
    "       'ps_car_01_cat_avg', 'ps_car_07_cat_avg', 'ps_car_03_cat_avg',\n",
    "       'ps_car_06_cat_avg', 'ps_car_04_cat_avg', 'ps_car_09_cat_avg',\n",
    "       'ps_car_02_cat_avg', 'ps_ind_02_cat_avg', 'ps_car_05_cat_avg',\n",
    "       'ps_car_08_cat_avg', 'ps_ind_04_cat_avg',\n",
    "       'ps_reg_01_plus_ps_car_02_cat_avg', 'ps_reg_01_plus_ps_car_04_cat_avg',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03_avg', 'logsum_mean_target_all',\n",
    "       'ps_ind_17_bin_avg', 'ps_ind_16_bin_avg', 'ps_ind_07_bin_avg', 'ps_ind_09_bin_avg',\n",
    "       'ps_ind_06_bin_avg', 'ps_ind_08_bin_avg', 'ps_ind_18_bin_avg', 'ps_ind_12_bin_avg',\n",
    "      ]\n",
    "\n",
    "# Same than FS5, but removing the variables that are encoded my mean target (cat and bin)\n",
    "feature_sets['FS6'] = ['ps_car_13', 'ps_reg_03', 'ps_ind_03', 'ps_ind_15',\n",
    "       'ps_reg_02', 'ps_car_14', 'ps_car_12', \n",
    "       'ps_reg_01', 'ps_car_15', 'ps_ind_01',\n",
    "       'ps_car_11', 'ps_calc_09', 'ps_calc_05',\n",
    "       'ps_ind_14',\n",
    "       'sum_ps_ind_03_ps_calc_02', 'diff_ps_ind_03_ps_ind_15',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03', 'ps_ind_05_cat_avg',\n",
    "       'ps_car_01_cat_avg', 'ps_car_07_cat_avg', 'ps_car_03_cat_avg',\n",
    "       'ps_car_06_cat_avg', 'ps_car_04_cat_avg', 'ps_car_09_cat_avg',\n",
    "       'ps_car_02_cat_avg', 'ps_ind_02_cat_avg', 'ps_car_05_cat_avg',\n",
    "       'ps_car_08_cat_avg', 'ps_ind_04_cat_avg',\n",
    "       'ps_reg_01_plus_ps_car_02_cat_avg', 'ps_reg_01_plus_ps_car_04_cat_avg',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03_avg', 'logsum_mean_target_all',\n",
    "       'ps_ind_17_bin_avg', 'ps_ind_16_bin_avg', 'ps_ind_07_bin_avg', 'ps_ind_09_bin_avg',\n",
    "       'ps_ind_06_bin_avg', 'ps_ind_08_bin_avg', 'ps_ind_18_bin_avg', 'ps_ind_12_bin_avg',\n",
    "      ]\n",
    "\n",
    "\n",
    "# Same than FS4, but adding cat vars\n",
    "feature_sets['FS7'] = ['ps_car_13', 'ps_reg_03', 'ps_ind_05_cat', 'ps_ind_03', 'ps_ind_15',\n",
    "       'ps_reg_02', 'ps_car_14', 'ps_car_12', 'ps_car_01_cat', 'ps_car_07_cat',\n",
    "       'ps_ind_17_bin', 'ps_car_03_cat', 'ps_reg_01', 'ps_car_15', 'ps_ind_01',\n",
    "       'ps_ind_16_bin', 'ps_ind_07_bin', 'ps_car_06_cat', 'ps_car_04_cat',\n",
    "       'ps_ind_06_bin', 'ps_car_09_cat', 'ps_car_02_cat', 'ps_ind_02_cat',\n",
    "       'ps_car_11', 'ps_car_05_cat', 'ps_calc_09', 'ps_calc_05',\n",
    "       'ps_ind_08_bin', 'ps_car_08_cat', 'ps_ind_09_bin', 'ps_ind_04_cat',\n",
    "       'ps_ind_18_bin', 'ps_ind_12_bin', 'ps_ind_14',\n",
    "       'sum_ps_ind_03_ps_calc_02', 'diff_ps_ind_03_ps_ind_15',\n",
    "       'ps_reg_01_plus_ps_car_02_cat', 'ps_reg_01_plus_ps_car_04_cat',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03', \n",
    "       'ps_ind_05_cat_bayes_prob', 'ps_car_01_cat_bayes_prob',\n",
    "       'ps_car_07_cat_bayes_prob', 'ps_car_03_cat_bayes_prob',\n",
    "       'ps_car_06_cat_bayes_prob', 'ps_car_04_cat_bayes_prob',\n",
    "       'ps_car_09_cat_bayes_prob', 'ps_car_02_cat_bayes_prob',\n",
    "       'ps_ind_02_cat_bayes_prob', 'ps_car_05_cat_bayes_prob',\n",
    "       'ps_car_08_cat_bayes_prob', 'ps_ind_04_cat_bayes_prob',\n",
    "       'ps_reg_01_plus_ps_car_02_cat_bayes_prob',\n",
    "       'ps_reg_01_plus_ps_car_04_cat_bayes_prob',\n",
    "       'ps_ind_02_cat_plus_ps_ind_03_bayes_prob']\n",
    "\n",
    "#\n",
    "use_as_cat_features = ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_reg_01',\n",
    "                            'ps_reg_02', 'ps_car_11']\n",
    "new_mean_encoded_features = ['ps_ind_01_avg', 'ps_ind_03_avg', 'ps_ind_14_avg', 'ps_ind_15_avg', 'ps_reg_01_avg',\n",
    "                            'ps_reg_02_avg', 'ps_car_11_avg']\n",
    "\n",
    "feature_sets['FS8'] = feature_sets['FS1'] + new_mean_encoded_features\n",
    "feature_sets['FS9'] = feature_sets['FS2'] + new_mean_encoded_features\n",
    "feature_sets['FS10'] = feature_sets['FS3'] + new_mean_encoded_features\n",
    "feature_sets['FS11'] = feature_sets['FS6'] + new_mean_encoded_features\n",
    "feature_sets['FS12'] = feature_sets['FS7'] + new_mean_encoded_features\n",
    "\n",
    "feature_sets['FS13'] = [f for f in feature_sets['FS8'] if f not in use_as_cat_features]\n",
    "feature_sets['FS14'] = [f for f in feature_sets['FS9'] if f not in use_as_cat_features]\n",
    "feature_sets['FS15'] = [f for f in feature_sets['FS10'] if f not in use_as_cat_features]\n",
    "feature_sets['FS16'] = [f for f in feature_sets['FS11'] if f not in use_as_cat_features]\n",
    "feature_sets['FS17'] = [f for f in feature_sets['FS12'] if f not in use_as_cat_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb1_FS13',\n",
       " 'xgb2_FS13',\n",
       " 'lgb1_FS15',\n",
       " 'lgb2_FS15',\n",
       " 'catboost1_FS15',\n",
       " 'catboost2_FS15',\n",
       " 'catboost3_FS15',\n",
       " 'RForest1_FS16',\n",
       " 'RForest2_FS14',\n",
       " 'RForest3_FS14',\n",
       " 'LR1_FS14',\n",
       " 'xgb3_FS17',\n",
       " 'lgb3_FS17']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost\n",
    "xgb1_params = {}\n",
    "xgb1_params['objective'] = \"binary:logistic\"\n",
    "xgb1_params['n_estimators'] = 400 # MAX_ROUNDS\n",
    "xgb1_params['learning_rate'] = 0.07\n",
    "xgb1_params['max_depth'] = 4\n",
    "xgb1_params['subsample'] = 0.80\n",
    "xgb1_params['colsample_bytree'] = 0.80\n",
    "xgb1_params['min_child_weight'] = 6\n",
    "xgb1_params['gamma'] = 10\n",
    "xgb1_params['reg_alpha'] = 8\n",
    "xgb1_params['reg_lambda'] = 1.5\n",
    "xgb1_params['scale_pos_weight'] = 1.6\n",
    "#xgb1_params['max_delta_step'] = 0\n",
    "xgb1_params['seed'] = models_seed\n",
    "xgb1 = XGBClassifier(**xgb1_params)\n",
    "\n",
    "xgb2_params = {}\n",
    "xgb2_params['objective'] = \"count:poisson\"\n",
    "xgb2_params['n_estimators'] = 400 # MAX_ROUNDS\n",
    "xgb2_params['learning_rate'] = 0.06\n",
    "xgb2_params['max_depth'] = 4\n",
    "xgb2_params['subsample'] = 0.85\n",
    "xgb2_params['colsample_bytree'] = 0.82\n",
    "xgb2_params['min_child_weight'] = 8\n",
    "xgb2_params['gamma'] = 9\n",
    "xgb2_params['reg_alpha'] = 8\n",
    "xgb2_params['reg_lambda'] = 1.5\n",
    "xgb2_params['scale_pos_weight'] = 1.6\n",
    "xgb2_params['seed'] = models_seed\n",
    "xgb2 = XGBClassifier(**xgb2_params)\n",
    "\n",
    "xgb3_params = {}\n",
    "xgb3_params['objective'] = \"binary:logistic\"\n",
    "xgb3_params['n_estimators'] = 420 # MAX_ROUNDS\n",
    "xgb3_params['learning_rate'] = 0.05\n",
    "xgb3_params['max_depth'] = 5\n",
    "xgb3_params['subsample'] = 0.80\n",
    "xgb3_params['colsample_bytree'] = 0.80\n",
    "xgb3_params['min_child_weight'] = 6\n",
    "xgb3_params['gamma'] = 10\n",
    "xgb3_params['reg_alpha'] = 8\n",
    "xgb3_params['reg_lambda'] = 1.5\n",
    "xgb3_params['scale_pos_weight'] = 1.6\n",
    "#xgb1_params['max_delta_step'] = 0\n",
    "xgb3_params['seed'] = models_seed\n",
    "xgb3 = XGBClassifier(**xgb3_params)\n",
    "\n",
    "# LightGBM\n",
    "lgb1_params = {}\n",
    "lgb1_params['learning_rate'] = 0.02\n",
    "lgb1_params['n_estimators'] = 650\n",
    "lgb1_params['max_bin'] = 10\n",
    "lgb1_params['subsample'] = 0.8\n",
    "lgb1_params['subsample_freq'] = 10\n",
    "lgb1_params['colsample_bytree'] = 0.8   \n",
    "lgb1_params['min_child_samples'] = 500\n",
    "lgb1_params['seed'] = models_seed\n",
    "lgb1 = LGBMClassifier(**lgb1_params)\n",
    "\n",
    "lgb2_params = {}\n",
    "lgb2_params['n_estimators'] = 1090\n",
    "lgb2_params['learning_rate'] = 0.02\n",
    "lgb2_params['colsample_bytree'] = 0.3   \n",
    "lgb2_params['subsample'] = 0.7\n",
    "lgb2_params['subsample_freq'] = 2\n",
    "lgb2_params['num_leaves'] = 16\n",
    "lgb2_params['seed'] = 99\n",
    "lgb2 = LGBMClassifier(**lgb2_params)\n",
    "\n",
    "# LightGBM\n",
    "lgb3_params = {}\n",
    "lgb3_params['learning_rate'] = 0.02\n",
    "lgb3_params['n_estimators'] = 650\n",
    "lgb3_params['max_bin'] = 10\n",
    "lgb3_params['subsample'] = 0.8\n",
    "lgb3_params['subsample_freq'] = 10\n",
    "lgb3_params['colsample_bytree'] = 0.8   \n",
    "lgb3_params['min_child_samples'] = 500\n",
    "lgb3_params['seed'] = models_seed\n",
    "lgb3 = LGBMClassifier(**lgb3_params)\n",
    "\n",
    "# CatBoost\n",
    "cat1_params = {}\n",
    "cat1_params['iterations'] = 500\n",
    "cat1_params['depth'] = 8\n",
    "cat1_params['rsm'] = 0.95\n",
    "cat1_params['learning_rate'] = 0.03\n",
    "cat1_params['l2_leaf_reg'] = 3.5  \n",
    "cat1_params['border_count'] = 8\n",
    "cat1_params['gradient_iterations'] = 4\n",
    "cat1_params['random_seed'] = 0\n",
    "catboost1 = CatBoostClassifier(**cat1_params)\n",
    "\n",
    "cat2_params = {}\n",
    "cat2_params['iterations'] = 500\n",
    "cat2_params['learning_rate'] = 0.02\n",
    "cat2_params['depth'] = 7\n",
    "cat2_params['loss_function'] = 'Logloss'\n",
    "cat2_params['random_seed'] = 0 \n",
    "cat2_params['od_type'] = 'Iter'\n",
    "cat2_params['od_wait'] = 100\n",
    "catboost2 = CatBoostClassifier(**cat2_params) \n",
    "\n",
    "cat3_params = {}\n",
    "cat3_params['iterations'] = 500\n",
    "cat3_params['learning_rate'] = 0.05\n",
    "cat3_params['depth'] = 6\n",
    "cat3_params['l2_leaf_reg'] = 14\n",
    "cat3_params['loss_function'] = 'Logloss'\n",
    "cat3_params['random_seed'] = 0\n",
    "catboost3 = CatBoostClassifier(**cat3_params)\n",
    "\n",
    "# Random Forest\n",
    "# RF1: Gini for full training set: 0.274193075438 | Mean gini folds: Train: 0.347153499445 Val: 0.27428136885\n",
    "rf1_params = {'max_leaf_nodes': None, 'min_weight_fraction_leaf': 0, 'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 800, 'min_impurity_decrease': 0.0, 'n_estimators': 110, 'min_samples_split': 800, 'max_features': 'auto', 'class_weight': None, 'min_impurity_split': None}\n",
    "rf1 = RandomForestClassifier(**rf1_params)\n",
    "\n",
    "# RF2: Gini for full training set: 0.276003725654 | Mean gini folds: Train: 0.383706123485 Val: 0.276099308539    \n",
    "rf2_params = {'max_leaf_nodes': None, 'min_weight_fraction_leaf': 0, 'criterion': 'gini', 'max_depth': 17, 'min_samples_leaf': 500, 'min_impurity_decrease': 0.0, 'n_estimators': 110, 'min_samples_split': 500, 'max_features': 'auto', 'class_weight': None, 'min_impurity_split': None}\n",
    "rf2 = RandomForestClassifier(**rf2_params)   \n",
    "    \n",
    "# RF3: Gini for full training set: 0.276579579526 | Mean gini folds: Train: 0.54986082691 Val: 0.27660726808\n",
    "# Best random forest, but it seems to be high overfitting\n",
    "rf3_params = {'max_leaf_nodes': None, 'min_weight_fraction_leaf': 0, 'criterion': 'gini', 'max_depth': 16, 'min_samples_leaf': 100, 'min_impurity_decrease': 0.0, 'n_estimators': 110, 'min_samples_split': 300, 'max_features': 'auto', 'class_weight': None, 'min_impurity_split': None}\n",
    "rf3 = RandomForestClassifier(**rf3_params) \n",
    "\n",
    "# ExtraTree  Gini for full training set: 0.262197683408 | Mean gini folds: Train: 0.350928110376 Val: 0.262341618713\n",
    "# Consider not using it because performance is not very good\n",
    "#et1_params = {'max_leaf_nodes': None, 'min_weight_fraction_leaf': 0, 'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 50, 'min_impurity_decrease': 0.0, 'n_estimators': 11, 'min_samples_split': 50, 'max_features': 'auto', 'class_weight': None, 'min_impurity_split': None}\n",
    "#et1 = ExtraTreesClassifier(**et1_params)\n",
    "\n",
    "# Logistic Regression\n",
    "lr1_params = {'C': 10, 'class_weight': \"balanced\"}\n",
    "lr1 = LogisticRegression(**lr1_params)\n",
    "\n",
    "# Put all models in list\n",
    "models = [xgb1, xgb2, lgb1, lgb2, catboost1, catboost2, catboost3, rf1, rf2, rf3, lr1, xgb3, lgb3]\n",
    "models_params = [xgb1_params, xgb2_params, lgb1_params, lgb2_params, cat1_params, cat2_params, cat3_params, rf1_params, rf2_params, rf3_params, lr1_params, xgb3_params, lgb3_params]\n",
    "models_names = ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'catboost1', 'catboost2', 'catboost3', 'RForest1', 'RForest2', 'RForest3', 'LR1', 'xgb3', 'lgb3']\n",
    "model_weights = [0.25, 0.10, 0.15, 0.20, 0.05, 0.03, 0.04, 0.02, 0.02, 0.02, 0.01, 0.05, 0.06]\n",
    "models_train = [True, True, True, True, True, True, True, True, True,  True, True, True, True]\n",
    "models_feature_sets = ['FS13', 'FS13', 'FS15', 'FS15', 'FS15', 'FS15', 'FS15', 'FS16', 'FS14', 'FS14', 'FS14', 'FS17', 'FS17']\n",
    "for m in range(len(models_names)):\n",
    "    models_names[m] = models_names[m] + '_' + models_feature_sets[m]\n",
    "models_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data set train_meta  and test_meta\n",
    "train_meta = train_df[['id', 'target']].copy()\n",
    "test_meta = pd.DataFrame()\n",
    "test_meta['id'] = id_test\n",
    "for m in models_names:\n",
    "    train_meta[m] = 0\n",
    "    test_meta[m] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_null_feature = ['ps_reg_03', 'ps_car_14']\n",
    "features_scale = ['ps_ind_01', 'ps_ind_14', 'ps_ind_15', 'ps_reg_01', 'ps_reg_02', 'ps_car_11', 'ps_car_12', 'ps_car_13',\n",
    "                 'ps_car_14', 'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', \n",
    "                 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12',\n",
    "                 'ps_calc_13', 'ps_calc_14', 'sum_ps_ind_03_ps_calc_02', 'diff_ps_ind_03_ps_ind_15', 'ps_ind_03']\n",
    "def get_data(X, fs, replace_null, scale_features):\n",
    "    X_new = X[feature_sets[fs]].copy()\n",
    "    \n",
    "    for c in X_new.columns:\n",
    "        if replace_null:\n",
    "            if X_new[c].isnull().sum() > 0:\n",
    "                if 'cat' in c:\n",
    "                    X_new[c].fillna(-1, inplace=True)\n",
    "                else:\n",
    "                    # Create new column indicating is null if in list\n",
    "                    if c in create_null_feature:\n",
    "                        X_new[c + 'is_null'] = 0\n",
    "                        X_new.loc[X_new[c].isnull(), c + 'is_null'] = 1\n",
    "                    if c in train_df.columns:\n",
    "                        median = pd.concat([train_df[c], test_df[c]], axis=0).median()\n",
    "                    else:\n",
    "                        print(c)\n",
    "                        median = X_new[c].median()\n",
    "                    X_new[c].fillna(median, inplace=True)\n",
    "                    X_new[c].fillna(median, inplace=True)\n",
    "        if scale_features:\n",
    "            if c in features_scale:\n",
    "                scaler = StandardScaler().fit(X_new[c].values.reshape(-1, 1))\n",
    "                X_new[c] = scaler.transform(X_new[c].values.reshape(-1,1))\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Train model xgb1_FS13\n",
      "TrainGini =  0.339645698847\n",
      "Val Gini =  0.290033557842\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb2_FS13\n",
      "TrainGini =  0.311189536604\n",
      "Val Gini =  0.286734088612\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.390182098862\n",
      "Val Gini =  0.289275081848\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.368686094339\n",
      "Val Gini =  0.289720803583\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost1_FS15\n",
      "TrainGini =  0.36927305957\n",
      "Val Gini =  0.289533530372\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost2_FS15\n",
      "TrainGini =  0.320250777418\n",
      "Val Gini =  0.285200536284\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost3_FS15\n",
      "TrainGini =  0.33788095221\n",
      "Val Gini =  0.285491667104\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest1_FS16\n",
      "TrainGini =  0.34775365851\n",
      "Val Gini =  0.281575461697\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest2_FS14\n",
      "TrainGini =  0.380559088238\n",
      "Val Gini =  0.284272335779\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest3_FS14\n",
      "TrainGini =  0.540116596157\n",
      "Val Gini =  0.282537750965\n",
      "---------NEXT MODEL---------\n",
      "Train model LR1_FS14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.278242581816\n",
      "Val Gini =  0.274023528713\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb3_FS17\n",
      "TrainGini =  0.354323638857\n",
      "Val Gini =  0.291120190929\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb3_FS17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.391927359186\n",
      "Val Gini =  0.292096838225\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.363848549278\n",
      "Mean val error: 0.286278105535\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Fold  1\n",
      "Train model xgb1_FS13\n",
      "TrainGini =  0.342643054661\n",
      "Val Gini =  0.286752754451\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb2_FS13\n",
      "TrainGini =  0.313145293421\n",
      "Val Gini =  0.282037181\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.391115279839\n",
      "Val Gini =  0.283624444329\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.369698180545\n",
      "Val Gini =  0.282872747509\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost1_FS15\n",
      "TrainGini =  0.368093006576\n",
      "Val Gini =  0.281998627705\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost2_FS15\n",
      "TrainGini =  0.318671431317\n",
      "Val Gini =  0.276202850735\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost3_FS15\n",
      "TrainGini =  0.339298293557\n",
      "Val Gini =  0.282348066666\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest1_FS16\n",
      "TrainGini =  0.346823891946\n",
      "Val Gini =  0.270467917567\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest2_FS14\n",
      "TrainGini =  0.382020798863\n",
      "Val Gini =  0.272719411485\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest3_FS14\n",
      "TrainGini =  0.53979729007\n",
      "Val Gini =  0.276224920226\n",
      "---------NEXT MODEL---------\n",
      "Train model LR1_FS14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.2795207792\n",
      "Val Gini =  0.266240384461\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb3_FS17\n",
      "TrainGini =  0.354341623422\n",
      "Val Gini =  0.285153857623\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb3_FS17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.39517743703\n",
      "Val Gini =  0.283785095111\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.364642027727\n",
      "Mean val error: 0.279263712221\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Fold  2\n",
      "Train model xgb1_FS13\n",
      "TrainGini =  0.342276569451\n",
      "Val Gini =  0.285324136683\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb2_FS13\n",
      "TrainGini =  0.314758473702\n",
      "Val Gini =  0.283362427395\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.391394099494\n",
      "Val Gini =  0.283868964802\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.371535573743\n",
      "Val Gini =  0.283521257806\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost1_FS15\n",
      "TrainGini =  0.370888973089\n",
      "Val Gini =  0.283948249407\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost2_FS15\n",
      "TrainGini =  0.321001553318\n",
      "Val Gini =  0.28074990522\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost3_FS15\n",
      "TrainGini =  0.339263095019\n",
      "Val Gini =  0.283843684546\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest1_FS16\n",
      "TrainGini =  0.347997007358\n",
      "Val Gini =  0.274752521122\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest2_FS14\n",
      "TrainGini =  0.381334761921\n",
      "Val Gini =  0.276321994641\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest3_FS14\n",
      "TrainGini =  0.540778758334\n",
      "Val Gini =  0.274565051005\n",
      "---------NEXT MODEL---------\n",
      "Train model LR1_FS14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.279213182432\n",
      "Val Gini =  0.271446067775\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb3_FS17\n",
      "TrainGini =  0.35519593559\n",
      "Val Gini =  0.284502667001\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb3_FS17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.396201670848\n",
      "Val Gini =  0.281879375301\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.365526127254\n",
      "Mean val error: 0.280622023285\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Fold  3\n",
      "Train model xgb1_FS13\n",
      "TrainGini =  0.341562454051\n",
      "Val Gini =  0.288825876523\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb2_FS13\n",
      "TrainGini =  0.312033682601\n",
      "Val Gini =  0.288015225558\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.389505519889\n",
      "Val Gini =  0.288147703934\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.371221848219\n",
      "Val Gini =  0.290357895094\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost1_FS15\n",
      "TrainGini =  0.369648081534\n",
      "Val Gini =  0.287061503411\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost2_FS15\n",
      "TrainGini =  0.31790110654\n",
      "Val Gini =  0.287745587235\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost3_FS15\n",
      "TrainGini =  0.339297450188\n",
      "Val Gini =  0.286447771724\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest1_FS16\n",
      "TrainGini =  0.346420678427\n",
      "Val Gini =  0.283434102648\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest2_FS14\n",
      "TrainGini =  0.381672841634\n",
      "Val Gini =  0.284755595248\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest3_FS14\n",
      "TrainGini =  0.539194260886\n",
      "Val Gini =  0.285157848576\n",
      "---------NEXT MODEL---------\n",
      "Train model LR1_FS14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.276919957768\n",
      "Val Gini =  0.277979977497\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb3_FS17\n",
      "TrainGini =  0.351814380671\n",
      "Val Gini =  0.289066706113\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb3_FS17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.394409470016\n",
      "Val Gini =  0.292001897615\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.363969364033\n",
      "Mean val error: 0.286845976244\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Fold  4\n",
      "Train model xgb1_FS13\n",
      "TrainGini =  0.341884586765\n",
      "Val Gini =  0.281569354771\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb2_FS13\n",
      "TrainGini =  0.315099201273\n",
      "Val Gini =  0.281045135241\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb1_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.388584199026\n",
      "Val Gini =  0.27786578947\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb2_FS15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.372406327194\n",
      "Val Gini =  0.281606667243\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost1_FS15\n",
      "TrainGini =  0.368831598124\n",
      "Val Gini =  0.279469695197\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost2_FS15\n",
      "TrainGini =  0.322418405137\n",
      "Val Gini =  0.278407581997\n",
      "---------NEXT MODEL---------\n",
      "Train model catboost3_FS15\n",
      "TrainGini =  0.342122929345\n",
      "Val Gini =  0.282156209253\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest1_FS16\n",
      "TrainGini =  0.348905089096\n",
      "Val Gini =  0.271041361267\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest2_FS14\n",
      "TrainGini =  0.382183141674\n",
      "Val Gini =  0.272481334087\n",
      "---------NEXT MODEL---------\n",
      "Train model RForest3_FS14\n",
      "TrainGini =  0.535876774678\n",
      "Val Gini =  0.271588025357\n",
      "---------NEXT MODEL---------\n",
      "Train model LR1_FS14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.279717770854\n",
      "Val Gini =  0.266920714639\n",
      "---------NEXT MODEL---------\n",
      "Train model xgb3_FS17\n",
      "TrainGini =  0.356847747636\n",
      "Val Gini =  0.280519605373\n",
      "---------NEXT MODEL---------\n",
      "Train model lgb3_FS17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function booster is deprecated; Use attribute booster_ instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainGini =  0.394397613468\n",
      "Val Gini =  0.275539602233\n",
      "---------NEXT MODEL---------\n",
      "Mean train error: 0.365328875713\n",
      "Mean val error: 0.276939313548\n",
      "---------------------------------------END FOLD------------------------------------------\n",
      "\n",
      "Gini for full training set with weighting models:\n",
      "0.288048451147\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "y_valid_pred = 0*y\n",
    "\n",
    "\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = StratifiedKFold(n_splits = K, random_state = kfold_seed, shuffle = True)\n",
    "# Also try with stratified\n",
    "np.random.seed(np_seed)\n",
    "\n",
    "# Run CV\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X,y)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    # Enocode data\n",
    "    for f in (f_cats + f_bin + use_as_cat_features):\n",
    "        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "        \n",
    "        X_train[f + '_bayes_prob'] = 0\n",
    "        X_valid[f + '_bayes_prob'] = 0\n",
    "        X_test[f + '_bayes_prob'] = 0\n",
    "        for value in train_df[f].unique():\n",
    "            prior = len(X_train[X_train[f] == value]) / len(X_train)\n",
    "            if math.isnan(value):\n",
    "                cond = len(X_train[(y_train==1) & (X_train[f].isnull())]) / (len(X_train[X_train[f].isnull()]) + 1)\n",
    "            else:\n",
    "                cond = len(X_train[(y_train==1) & (X_train[f] == value)]) / (len(X_train[X_train[f] == value]) + 1)\n",
    "            X_train.loc[X_train[f] == value, f + '_bayes_prob'] = prior * cond\n",
    "            X_valid.loc[X_valid[f] == value, f + '_bayes_prob'] = prior * cond\n",
    "            X_test.loc[X_test[f] == value, f + '_bayes_prob'] = prior * cond\n",
    "    gc.collect()\n",
    "            \n",
    "            \n",
    "        \n",
    "    all_mean_target_col = X_train.columns[X_train.columns.str.endswith('_avg')]\n",
    "    ind_mean_target_col = X_train.columns[(X_train.columns.str.endswith('_avg')) & (X_train.columns.str.startswith('ps_ind'))]\n",
    "    car_mean_target_col = X_train.columns[(X_train.columns.str.endswith('_avg')) & (X_train.columns.str.startswith('ps_car'))]\n",
    "\n",
    "    X_train['logsum_mean_target_all'] = np.log(X_train[all_mean_target_col]).sum(axis=1)\n",
    "    X_valid['logsum_mean_target_all'] = np.log(X_valid[all_mean_target_col]).sum(axis=1)\n",
    "    X_test['logsum_mean_target_all'] = np.log(X_test[all_mean_target_col]).sum(axis=1)\n",
    "    gc.collect()\n",
    "        \n",
    "    \n",
    "        \n",
    "    l_gini_train = []\n",
    "    l_gini_val = []\n",
    "    for model,model_params,model_name,model_weight,model_train, model_fs in zip(models, models_params, models_names, model_weights, models_train, models_feature_sets):\n",
    "        # if train is false load the model which is already trained in the same fold, otherwise train the model\n",
    "        model_filename = 'models/' + model_name + '_StratifiedFold_' + str(i) + '_' + model_fs\n",
    "        replace_null = False\n",
    "        scale_features = True if 'LR' in model_name else False\n",
    "        if ('RForest' in model_name) or ('ExtraTree' in model_name) or ('LR' in model_name):\n",
    "            replace_null = True\n",
    "        if not model_train:\n",
    "            print('Load model ' + model_name + ': ' + model_filename)\n",
    "            if 'xgb' in model_name:\n",
    "                booster = xgb.Booster()\n",
    "                booster.load_model(model_filename + '.txt')\n",
    "                model._Booster = booster\n",
    "                fit_model = model\n",
    "            elif 'lgb' in model_name:\n",
    "                booster = lightgbm.Booster(model_file=(model_filename + '.txt'))\n",
    "                model._Booster = booster\n",
    "                fit_model = model\n",
    "                fit_model.n_classes = 2\n",
    "            elif ('RForest' in model_name) or ('ExtraTree' in model_name) or ('LR' in model_name):\n",
    "                fit_model = pickle.load(open(model_filename + '.pickle', 'rb'))\n",
    "            else:\n",
    "                fit_model = model.load_model(model_filename)\n",
    "        else:\n",
    "            #Fit the base model to the training fold \n",
    "            print('Train model ' + model_name)\n",
    "            fit_model = model.fit(get_data(X_train, model_fs, replace_null, scale_features), y_train)\n",
    "            gc.collect()\n",
    "            if ('xgb' in model_name) or ('lgb' in model_name):\n",
    "                fit_model.booster().save_model(model_filename + '.txt')\n",
    "            elif ('RForest' in model_name) or ('ExtraTree' in model_name) or ('LR' in model_name):\n",
    "                pickle.dump(fit_model, open(model_filename + '.pickle', 'wb'))\n",
    "            else:\n",
    "                fit_model.save_model(model_filename + '.txt')\n",
    "        \n",
    "        # Train error\n",
    "        pred = fit_model.predict_proba(get_data(X_train, model_fs, replace_null, scale_features))[:,1]   \n",
    "        l_gini_train.append(eval_gini(y_train, pred))\n",
    "        print(\"TrainGini = \", str(l_gini_train[-1]))\n",
    "        gc.collect()\n",
    "        \n",
    "        # make predictions on the test fold. Store these predictions in train_meta to be used as features for the stacking model\n",
    "        pred = fit_model.predict_proba(get_data(X_valid, model_fs, replace_null, scale_features))[:,1]  \n",
    "        l_gini_val.append(eval_gini(y_valid, pred))\n",
    "        print(\"Val Gini = \", str(l_gini_val[-1]))\n",
    "        train_meta.loc[test_index, model_name] = pred\n",
    "        y_valid_pred.iloc[test_index] += pred * model_weight # Just to see the performance of weighting models in CV\n",
    "        gc.collect()\n",
    "        \n",
    "        # make predictions on the test_meta dataset using each base model immediately after it gets fit to each test fold\n",
    "        test_meta[model_name] += fit_model.predict_proba(get_data(X_test, model_fs, replace_null, scale_features))[:,1] \n",
    "        \n",
    "\n",
    "        print('---------NEXT MODEL---------')\n",
    "    print('Mean train error: ' + str(np.mean(l_gini_train)))\n",
    "    print('Mean val error: ' + str(np.mean(l_gini_val)))\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    gc.collect()\n",
    "        \n",
    "    print('---------------------------------------END FOLD------------------------------------------')\n",
    "    \n",
    "\n",
    "print( \"\\nGini for full training set with weighting models:\" )\n",
    "print(eval_gini(y, y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb1: 0.287451334445\n",
      "xgb2: 0.284310224357\n",
      "lgb1: 0.287070290858\n",
      "lgb2: 0.28703155883\n",
      "catboost1: 0.286721366645\n",
      "catboost2: 0.282411218983\n",
      "catboost3: 0.286345207361\n",
      "RForest1: 0.275126595919\n",
      "RForest2: 0.27678909239\n",
      "RForest3: 0.278819868962\n",
      "ExtraTree1: 0.253428329013\n",
      "LR1: 0.269399536795\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CV of each individual model\n",
    "for model_name in models_names:\n",
    "    print(model_name + ': ' + str(eval_gini(y, train_meta[model_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb1: 0.288046013899\n",
      "xgb2: 0.284421805069\n",
      "lgb1: 0.28742875413\n",
      "lgb2: 0.287559384506\n",
      "catboost1: 0.28591085648\n",
      "catboost2: 0.280604361918\n",
      "catboost3: 0.285791647191\n",
      "RForest1: 0.274808812288\n",
      "RForest2: 0.276497521477\n",
      "RForest3: 0.277853811327\n",
      "ExtraTree1: 0.254441026793\n",
      "LR1: 0.269969589946\n",
      "xgb3: 0.28542750874\n",
      "lgb3: 0.284953125548\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CV of each individual model\n",
    "for model_name in models_names:\n",
    "    print(model_name + ': ' + str(eval_gini(y, train_meta[model_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb1_FS5: 0.287915688782\n",
      "xgb2_FS5: 0.28325958655\n",
      "lgb1_FS5: 0.286849060973\n",
      "lgb2_FS5: 0.28755828612\n",
      "catboost1_FS5: 0.285065211649\n",
      "catboost2_FS5: 0.280509292351\n",
      "catboost3_FS5: 0.284982460779\n",
      "RForest1_FS6: 0.276564045724\n",
      "RForest2_FS6: 0.277222195712\n",
      "RForest3_FS6: 0.278051282622\n",
      "LR1_FS6: 0.269976807493\n",
      "xgb3_FS7: 0.287842153708\n",
      "lgb3_FS7: 0.28585938733\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CV of each individual model\n",
    "for model_name in models_names:\n",
    "    print(model_name + ': ' + str(eval_gini(y, train_meta[model_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb1_FS1: 0.287950400202\n",
      "xgb2_FS1: 0.283881437257\n",
      "lgb1_FS3: 0.287070290858\n",
      "lgb2_FS3: 0.28703155883\n",
      "catboost1_FS3: 0.286721366645\n",
      "catboost2_FS3: 0.282411218983\n",
      "catboost3_FS3: 0.286345207361\n",
      "RForest1_FS6: 0.276564045724\n",
      "RForest2_FS2: 0.278013491447\n",
      "RForest3_FS2: 0.277738732114\n",
      "LR1_FS2: 0.269995706215\n",
      "xgb3_FS7: 0.287842153708\n",
      "lgb3_FS7: 0.28585938733\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CV of each individual model\n",
    "for model_name in models_names:\n",
    "    print(model_name + ': ' + str(eval_gini(y, train_meta[model_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb1_FS8: 0.2879920387\n",
      "xgb2_FS8: 0.285282595016\n",
      "lgb1_FS10: 0.286896886789\n",
      "lgb2_FS10: 0.287497800179\n",
      "catboost1_FS10: 0.286377927927\n",
      "catboost2_FS10: 0.282642162016\n",
      "catboost3_FS10: 0.286157651112\n",
      "RForest1_FS11: 0.277588066695\n",
      "RForest2_FS9: 0.278774096048\n",
      "RForest3_FS9: 0.278536573152\n",
      "LR1_FS9: 0.271991521566\n",
      "xgb3_FS12: 0.287692509195\n",
      "lgb3_FS12: 0.285741965066\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CV of each individual model\n",
    "for model_name in models_names:\n",
    "    print(model_name + ': ' + str(eval_gini(y, train_meta[model_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb1_FS13: 0.286447930583\n",
      "xgb2_FS13: 0.284205215632\n",
      "lgb1_FS15: 0.284512375966\n",
      "lgb2_FS15: 0.285560618158\n",
      "catboost1_FS15: 0.284368005084\n",
      "catboost2_FS15: 0.281621820869\n",
      "catboost3_FS15: 0.284034533173\n",
      "RForest1_FS16: 0.276012539903\n",
      "RForest2_FS14: 0.277942529613\n",
      "RForest3_FS14: 0.277854771791\n",
      "LR1_FS14: 0.271304263\n",
      "xgb3_FS17: 0.286021841463\n",
      "lgb3_FS17: 0.285008053817\n"
     ]
    }
   ],
   "source": [
    "# Evaluate CV of each individual model\n",
    "for model_name in models_names:\n",
    "    print(model_name + ': ' + str(eval_gini(y, train_meta[model_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average the predictions per model\n",
    "for model_name in models_names:\n",
    "    test_meta[model_name] = test_meta[model_name] / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save train_meta and test_meta\n",
    "train_meta.to_csv('train_meta_stratified_7.csv', index=False)\n",
    "test_meta.to_csv('test_meta_stratified_7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>xgb1_FS1</th>\n",
       "      <th>xgb2_FS1</th>\n",
       "      <th>lgb1_FS3</th>\n",
       "      <th>lgb2_FS3</th>\n",
       "      <th>catboost1_FS3</th>\n",
       "      <th>catboost2_FS3</th>\n",
       "      <th>catboost3_FS3</th>\n",
       "      <th>RForest1_FS6</th>\n",
       "      <th>RForest2_FS2</th>\n",
       "      <th>RForest3_FS2</th>\n",
       "      <th>LR1_FS2</th>\n",
       "      <th>xgb3_FS7</th>\n",
       "      <th>lgb3_FS7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>0.035964</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.038288</td>\n",
       "      <td>0.037519</td>\n",
       "      <td>0.038638</td>\n",
       "      <td>0.043202</td>\n",
       "      <td>0.042935</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.511859</td>\n",
       "      <td>0.060376</td>\n",
       "      <td>0.049093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.034036</td>\n",
       "      <td>0.035894</td>\n",
       "      <td>0.030180</td>\n",
       "      <td>0.033580</td>\n",
       "      <td>0.032459</td>\n",
       "      <td>0.030049</td>\n",
       "      <td>0.034117</td>\n",
       "      <td>0.035344</td>\n",
       "      <td>0.032104</td>\n",
       "      <td>0.500058</td>\n",
       "      <td>0.048381</td>\n",
       "      <td>0.033164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031771</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.016545</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.017867</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.020899</td>\n",
       "      <td>0.021441</td>\n",
       "      <td>0.021695</td>\n",
       "      <td>0.405474</td>\n",
       "      <td>0.037623</td>\n",
       "      <td>0.018259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021677</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.014192</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>0.016284</td>\n",
       "      <td>0.015753</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>0.015809</td>\n",
       "      <td>0.307313</td>\n",
       "      <td>0.025344</td>\n",
       "      <td>0.016123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055609</td>\n",
       "      <td>0.032151</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>0.031474</td>\n",
       "      <td>0.029792</td>\n",
       "      <td>0.033922</td>\n",
       "      <td>0.034713</td>\n",
       "      <td>0.042312</td>\n",
       "      <td>0.039087</td>\n",
       "      <td>0.038936</td>\n",
       "      <td>0.511763</td>\n",
       "      <td>0.048221</td>\n",
       "      <td>0.029799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  xgb1_FS1  xgb2_FS1  lgb1_FS3  lgb2_FS3  catboost1_FS3  \\\n",
       "0   7       0  0.053054  0.036508  0.035964  0.035512       0.038288   \n",
       "1   9       0  0.052980  0.034036  0.035894  0.030180       0.033580   \n",
       "2  13       0  0.031771  0.021110  0.016545  0.017060       0.017867   \n",
       "3  16       0  0.021677  0.014408  0.014192  0.015797       0.014040   \n",
       "4  17       0  0.055609  0.032151  0.030553  0.031474       0.029792   \n",
       "\n",
       "   catboost2_FS3  catboost3_FS3  RForest1_FS6  RForest2_FS2  RForest3_FS2  \\\n",
       "0       0.037519       0.038638      0.043202      0.042935      0.039910   \n",
       "1       0.032459       0.030049      0.034117      0.035344      0.032104   \n",
       "2       0.020013       0.017954      0.020899      0.021441      0.021695   \n",
       "3       0.016284       0.015753      0.016483      0.015880      0.015809   \n",
       "4       0.033922       0.034713      0.042312      0.039087      0.038936   \n",
       "\n",
       "    LR1_FS2  xgb3_FS7  lgb3_FS7  \n",
       "0  0.511859  0.060376  0.049093  \n",
       "1  0.500058  0.048381  0.033164  \n",
       "2  0.405474  0.037623  0.018259  \n",
       "3  0.307313  0.025344  0.016123  \n",
       "4  0.511763  0.048221  0.029799  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>xgb1_FS8</th>\n",
       "      <th>xgb2_FS8</th>\n",
       "      <th>lgb1_FS10</th>\n",
       "      <th>lgb2_FS10</th>\n",
       "      <th>catboost1_FS10</th>\n",
       "      <th>catboost2_FS10</th>\n",
       "      <th>catboost3_FS10</th>\n",
       "      <th>RForest1_FS11</th>\n",
       "      <th>RForest2_FS9</th>\n",
       "      <th>RForest3_FS9</th>\n",
       "      <th>LR1_FS9</th>\n",
       "      <th>xgb3_FS12</th>\n",
       "      <th>lgb3_FS12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.198408</td>\n",
       "      <td>0.120515</td>\n",
       "      <td>0.123161</td>\n",
       "      <td>0.127104</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>0.121062</td>\n",
       "      <td>0.109951</td>\n",
       "      <td>0.111044</td>\n",
       "      <td>0.119232</td>\n",
       "      <td>1.885573</td>\n",
       "      <td>0.198050</td>\n",
       "      <td>0.126012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.171746</td>\n",
       "      <td>0.120112</td>\n",
       "      <td>0.116134</td>\n",
       "      <td>0.108445</td>\n",
       "      <td>0.113641</td>\n",
       "      <td>0.119195</td>\n",
       "      <td>0.108745</td>\n",
       "      <td>0.122865</td>\n",
       "      <td>0.122569</td>\n",
       "      <td>0.104134</td>\n",
       "      <td>1.985500</td>\n",
       "      <td>0.174861</td>\n",
       "      <td>0.126647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.176631</td>\n",
       "      <td>0.125183</td>\n",
       "      <td>0.112980</td>\n",
       "      <td>0.113138</td>\n",
       "      <td>0.117029</td>\n",
       "      <td>0.122265</td>\n",
       "      <td>0.117147</td>\n",
       "      <td>0.127744</td>\n",
       "      <td>0.121266</td>\n",
       "      <td>0.112151</td>\n",
       "      <td>1.892947</td>\n",
       "      <td>0.189148</td>\n",
       "      <td>0.121130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.115768</td>\n",
       "      <td>0.076765</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.073210</td>\n",
       "      <td>0.071573</td>\n",
       "      <td>0.077756</td>\n",
       "      <td>0.069903</td>\n",
       "      <td>0.079115</td>\n",
       "      <td>0.080836</td>\n",
       "      <td>0.070812</td>\n",
       "      <td>1.618653</td>\n",
       "      <td>0.112602</td>\n",
       "      <td>0.076683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.299675</td>\n",
       "      <td>0.196574</td>\n",
       "      <td>0.195166</td>\n",
       "      <td>0.191108</td>\n",
       "      <td>0.185926</td>\n",
       "      <td>0.187177</td>\n",
       "      <td>0.185918</td>\n",
       "      <td>0.205681</td>\n",
       "      <td>0.209545</td>\n",
       "      <td>0.201408</td>\n",
       "      <td>2.484105</td>\n",
       "      <td>0.298371</td>\n",
       "      <td>0.183803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  xgb1_FS8  xgb2_FS8  lgb1_FS10  lgb2_FS10  catboost1_FS10  \\\n",
       "0   0  0.198408  0.120515   0.123161   0.127104        0.113680   \n",
       "1   1  0.171746  0.120112   0.116134   0.108445        0.113641   \n",
       "2   2  0.176631  0.125183   0.112980   0.113138        0.117029   \n",
       "3   3  0.115768  0.076765   0.077930   0.073210        0.071573   \n",
       "4   4  0.299675  0.196574   0.195166   0.191108        0.185926   \n",
       "\n",
       "   catboost2_FS10  catboost3_FS10  RForest1_FS11  RForest2_FS9  RForest3_FS9  \\\n",
       "0        0.116788        0.121062       0.109951      0.111044      0.119232   \n",
       "1        0.119195        0.108745       0.122865      0.122569      0.104134   \n",
       "2        0.122265        0.117147       0.127744      0.121266      0.112151   \n",
       "3        0.077756        0.069903       0.079115      0.080836      0.070812   \n",
       "4        0.187177        0.185918       0.205681      0.209545      0.201408   \n",
       "\n",
       "    LR1_FS9  xgb3_FS12  lgb3_FS12  \n",
       "0  1.885573   0.198050   0.126012  \n",
       "1  1.985500   0.174861   0.126647  \n",
       "2  1.892947   0.189148   0.121130  \n",
       "3  1.618653   0.112602   0.076683  \n",
       "4  2.484105   0.298371   0.183803  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to see which features can be important to predict model error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute model errors\n",
    "\n",
    "for model_name in models_names:\n",
    "    train_meta['error_' + model_name] = train_meta['target'] - train_meta[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>xgb1</th>\n",
       "      <th>xgb2</th>\n",
       "      <th>lgb1</th>\n",
       "      <th>lgb2</th>\n",
       "      <th>catboost1</th>\n",
       "      <th>catboost2</th>\n",
       "      <th>catboost3</th>\n",
       "      <th>RForest1</th>\n",
       "      <th>...</th>\n",
       "      <th>error_lgb1</th>\n",
       "      <th>error_lgb2</th>\n",
       "      <th>error_catboost1</th>\n",
       "      <th>error_catboost2</th>\n",
       "      <th>error_catboost3</th>\n",
       "      <th>error_RForest1</th>\n",
       "      <th>error_RForest2</th>\n",
       "      <th>error_RForest3</th>\n",
       "      <th>error_ExtraTree1</th>\n",
       "      <th>error_LR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049214</td>\n",
       "      <td>0.038178</td>\n",
       "      <td>0.034175</td>\n",
       "      <td>0.037735</td>\n",
       "      <td>0.038324</td>\n",
       "      <td>0.037233</td>\n",
       "      <td>0.035312</td>\n",
       "      <td>0.039965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034175</td>\n",
       "      <td>-0.037735</td>\n",
       "      <td>-0.038324</td>\n",
       "      <td>-0.037233</td>\n",
       "      <td>-0.035312</td>\n",
       "      <td>-0.039965</td>\n",
       "      <td>-0.039412</td>\n",
       "      <td>-0.042134</td>\n",
       "      <td>-0.051316</td>\n",
       "      <td>-0.511523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044588</td>\n",
       "      <td>0.029538</td>\n",
       "      <td>0.039044</td>\n",
       "      <td>0.031475</td>\n",
       "      <td>0.033275</td>\n",
       "      <td>0.031951</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.028709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039044</td>\n",
       "      <td>-0.031475</td>\n",
       "      <td>-0.033275</td>\n",
       "      <td>-0.031951</td>\n",
       "      <td>-0.030196</td>\n",
       "      <td>-0.028709</td>\n",
       "      <td>-0.031707</td>\n",
       "      <td>-0.030566</td>\n",
       "      <td>-0.033507</td>\n",
       "      <td>-0.500043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.023046</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.018830</td>\n",
       "      <td>0.019173</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.019945</td>\n",
       "      <td>0.023162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014451</td>\n",
       "      <td>-0.018830</td>\n",
       "      <td>-0.019173</td>\n",
       "      <td>-0.020481</td>\n",
       "      <td>-0.019945</td>\n",
       "      <td>-0.023162</td>\n",
       "      <td>-0.022777</td>\n",
       "      <td>-0.026969</td>\n",
       "      <td>-0.029169</td>\n",
       "      <td>-0.406342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021312</td>\n",
       "      <td>0.014280</td>\n",
       "      <td>0.013969</td>\n",
       "      <td>0.015773</td>\n",
       "      <td>0.014852</td>\n",
       "      <td>0.016024</td>\n",
       "      <td>0.015207</td>\n",
       "      <td>0.016184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013969</td>\n",
       "      <td>-0.015773</td>\n",
       "      <td>-0.014852</td>\n",
       "      <td>-0.016024</td>\n",
       "      <td>-0.015207</td>\n",
       "      <td>-0.016184</td>\n",
       "      <td>-0.016653</td>\n",
       "      <td>-0.015075</td>\n",
       "      <td>-0.024602</td>\n",
       "      <td>-0.306179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062117</td>\n",
       "      <td>0.035459</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>0.033303</td>\n",
       "      <td>0.028968</td>\n",
       "      <td>0.035890</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029056</td>\n",
       "      <td>-0.033303</td>\n",
       "      <td>-0.028968</td>\n",
       "      <td>-0.035890</td>\n",
       "      <td>-0.036607</td>\n",
       "      <td>-0.044305</td>\n",
       "      <td>-0.041794</td>\n",
       "      <td>-0.043107</td>\n",
       "      <td>-0.034419</td>\n",
       "      <td>-0.511595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target      xgb1      xgb2      lgb1      lgb2  catboost1  catboost2  \\\n",
       "0   7       0  0.049214  0.038178  0.034175  0.037735   0.038324   0.037233   \n",
       "1   9       0  0.044588  0.029538  0.039044  0.031475   0.033275   0.031951   \n",
       "2  13       0  0.034072  0.023046  0.014451  0.018830   0.019173   0.020481   \n",
       "3  16       0  0.021312  0.014280  0.013969  0.015773   0.014852   0.016024   \n",
       "4  17       0  0.062117  0.035459  0.029056  0.033303   0.028968   0.035890   \n",
       "\n",
       "   catboost3  RForest1    ...      error_lgb1  error_lgb2  error_catboost1  \\\n",
       "0   0.035312  0.039965    ...       -0.034175   -0.037735        -0.038324   \n",
       "1   0.030196  0.028709    ...       -0.039044   -0.031475        -0.033275   \n",
       "2   0.019945  0.023162    ...       -0.014451   -0.018830        -0.019173   \n",
       "3   0.015207  0.016184    ...       -0.013969   -0.015773        -0.014852   \n",
       "4   0.036607  0.044305    ...       -0.029056   -0.033303        -0.028968   \n",
       "\n",
       "   error_catboost2  error_catboost3  error_RForest1  error_RForest2  \\\n",
       "0        -0.037233        -0.035312       -0.039965       -0.039412   \n",
       "1        -0.031951        -0.030196       -0.028709       -0.031707   \n",
       "2        -0.020481        -0.019945       -0.023162       -0.022777   \n",
       "3        -0.016024        -0.015207       -0.016184       -0.016653   \n",
       "4        -0.035890        -0.036607       -0.044305       -0.041794   \n",
       "\n",
       "   error_RForest3  error_ExtraTree1  error_LR1  \n",
       "0       -0.042134         -0.051316  -0.511523  \n",
       "1       -0.030566         -0.033507  -0.500043  \n",
       "2       -0.026969         -0.029169  -0.406342  \n",
       "3       -0.015075         -0.024602  -0.306179  \n",
       "4       -0.043107         -0.034419  -0.511595  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-04053cae48d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m# Set up folds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkfold_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;31m# Also try with stratified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "OPTIMIZE_ROUNDS = True\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = kfold_seed, shuffle = True)\n",
    "# Also try with stratified\n",
    "np.random.seed(np_seed)\n",
    "\n",
    "xgb1_params = {}\n",
    "xgb1_params['objective'] = \"binary:logistic\"\n",
    "xgb1_params['n_estimators'] = 400 # MAX_ROUNDS\n",
    "xgb1_params['learning_rate'] = 0.07\n",
    "xgb1_params['max_depth'] = 4\n",
    "xgb1_params['subsample'] = 0.80\n",
    "xgb1_params['colsample_bytree'] = 0.80\n",
    "xgb1_params['min_child_weight'] = 6\n",
    "xgb1_params['gamma'] = 10\n",
    "xgb1_params['reg_alpha'] = 8\n",
    "xgb1_params['reg_lambda'] = 1.5\n",
    "xgb1_params['scale_pos_weight'] = 1.6\n",
    "#xgb1_params['max_delta_step'] = 0\n",
    "xgb1_params['seed'] = models_seed\n",
    "xgb1 = XGBClassifier(**xgb1_params)\n",
    "\n",
    "params_xgb_reg = {\n",
    "    'objective': 'reg:linear',\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.80,\n",
    "    'min_child_weight': 6,\n",
    "    #'gamma': 10,\n",
    "    #'reg_alpha': 8,\n",
    "    #'reg_lambda', 1.5\n",
    "    'seed': models_seed\n",
    "}\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "\n",
    "feature_importances_1 = {}\n",
    "feature_importances_2 = {}\n",
    "for model,model_params,model_name,model_weight,model_train, model_fs in zip(models, models_params, models_names, model_weights, models_train, models_feature_sets):\n",
    "    print('##### ' + model_name + ' #####')\n",
    "    feature_importances_1[model_name] = []\n",
    "    feature_importances_2[model_name] = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "        \n",
    "        \n",
    "        # Create data for this fold\n",
    "        target_train, target_valid = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "        y_train, y_valid = train_meta.iloc[train_index]['error_' + model_name].copy(), train_meta.iloc[test_index]['error_' + model_name].copy()\n",
    "        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "        # CHECK that we are using the correspondent columns!\n",
    "        print( \"\\nFold \", i)\n",
    "    \n",
    "        # Enocode data\n",
    "        for f in f_cats:\n",
    "            X_train[f + \"_avg\"], X_valid[f + \"_avg\"], remove = target_encode(\n",
    "                                                            trn_series=X_train[f],\n",
    "                                                            val_series=X_valid[f],\n",
    "                                                            tst_series=X_train[f].iloc[0:20],\n",
    "                                                            target=target_train,\n",
    "                                                            min_samples_leaf=200,\n",
    "                                                            smoothing=10,\n",
    "                                                            noise_level=0\n",
    "                                                            )\n",
    "\n",
    "        all_mean_target_col = X_train.columns[X_train.columns.str.endswith('_avg')]\n",
    "        ind_mean_target_col = X_train.columns[(X_train.columns.str.endswith('_avg')) & (X_train.columns.str.startswith('ps_ind'))]\n",
    "        car_mean_target_col = X_train.columns[(X_train.columns.str.endswith('_avg')) & (X_train.columns.str.startswith('ps_car'))]\n",
    "\n",
    "        X_train['logsum_mean_target_all'] = np.log(X_train[all_mean_target_col]).sum(axis=1)\n",
    "        X_valid['logsum_mean_target_all'] = np.log(X_valid[all_mean_target_col]).sum(axis=1)\n",
    "        #X_test['logsum_mean_target_all'] = np.log(X_test[all_mean_target_col]).sum(axis=1)\n",
    "        \n",
    "        # Try with XGBoostRegression\n",
    "        if OPTIMIZE_ROUNDS:\n",
    "            eval_set=[(X_valid[feature_sets['FS2']], y_valid)]\n",
    "            fit_model = xgb_reg.fit( X_train[feature_sets['FS2']], y_train, \n",
    "                                   eval_set=eval_set,\n",
    "                                   eval_metric='rmse',\n",
    "                                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                                   verbose=50\n",
    "                                 )\n",
    "            print( \"  Best N trees = \", fit_model.best_ntree_limit )\n",
    "            print( \"  Best rmse = \", fit_model.best_score )\n",
    "            \n",
    "        else:\n",
    "            fit_model = xgb_reg.fit( X_train[feature_sets['FS2']], y_train)\n",
    "        # Store feature importance \n",
    "        importances_1 = sorted(fit_model.booster().get_score(importance_type='gain').items(), key=operator.itemgetter(1), reverse=True)\n",
    "        importances_2 = sorted(fit_model.booster().get_score(importance_type='weight').items(), key=operator.itemgetter(1), reverse=True)\n",
    "        feature_importances_1[model_name].append(importances_1)\n",
    "        feature_importances_2[model_name].append(importances_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "\n",
    "#plt.figure()\n",
    "#df.plot()\n",
    "#df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n",
    "#plt.gcf().savefig('features_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to predict the error for the test set, and include it as a feature for the Stacking model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extra_features = ['logsum_mean_target_all', 'ps_car_13','ps_reg_03','sum_ps_ind_03_ps_calc_02','ps_ind_02_cat_plus_ps_ind_03_avg',\n",
    "                  'ps_ind_15','ps_ind_05_cat_avg', 'ps_car_04_cat_avg', 'ps_ind_17_bin', 'ps_ind_17_bin', 'ps_ind_03',\n",
    "                 'ps_ind_18_bin','ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_05_cat_avg', 'ps_ind_18_bin', 'ps_car_03_cat']\n",
    "extra_features = [ 'ps_car_13', 'ps_reg_03', 'sum_ps_ind_03_ps_calc_02', 'ps_ind_17_bin',\n",
    "                 'ps_ind_15','ps_ind_03',  'ps_ind_18_bin', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_car_03_cat']\n",
    "for c in extra_features:\n",
    "    train_meta[c] = train_df[c]\n",
    "    test_meta[c] = test_df[c]\n",
    "#models_use = models_names\n",
    "models_use = ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'catboost3', 'RForest3', 'LR1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.265021\n",
      "Multiple eval metrics have been passed: 'validation_0-gini' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-gini hasn't improved in 25 rounds.\n",
      "[20]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.283966\n",
      "[40]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.289497\n",
      "[60]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.290483\n",
      "[80]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.291465\n",
      "[100]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.29187\n",
      "[120]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.292166\n",
      "[140]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.292685\n",
      "[160]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.293005\n",
      "[180]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.293\n",
      "Stopping. Best iteration:\n",
      "[158]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.293339\n",
      "\n",
      "Val Gini =  0.292886457437\n",
      "[0]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.265757\n",
      "Multiple eval metrics have been passed: 'validation_0-gini' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-gini hasn't improved in 25 rounds.\n",
      "[20]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.277575\n",
      "[40]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.281941\n",
      "[60]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.287133\n",
      "[80]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.287467\n",
      "[100]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.287971\n",
      "[120]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.288058\n",
      "[140]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.28821\n",
      "[160]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.288474\n",
      "Stopping. Best iteration:\n",
      "[135]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.288709\n",
      "\n",
      "Val Gini =  0.288473921561\n",
      "[0]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.258017\n",
      "Multiple eval metrics have been passed: 'validation_0-gini' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-gini hasn't improved in 25 rounds.\n",
      "[20]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.279601\n",
      "[40]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.281286\n",
      "[60]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.284159\n",
      "[80]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.284911\n",
      "[100]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.287134\n",
      "Stopping. Best iteration:\n",
      "[91]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.287971\n",
      "\n",
      "Val Gini =  0.286977564281\n",
      "[0]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.258693\n",
      "Multiple eval metrics have been passed: 'validation_0-gini' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-gini hasn't improved in 25 rounds.\n",
      "[20]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.280255\n",
      "[40]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.284797\n",
      "[60]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.288414\n",
      "[80]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.28965\n",
      "[100]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.290767\n",
      "[120]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.290974\n",
      "[140]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.291256\n",
      "[160]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.291773\n",
      "[180]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.291199\n",
      "Stopping. Best iteration:\n",
      "[174]\tvalidation_0-error:0.036449\tvalidation_0-gini:-0.291926\n",
      "\n",
      "Val Gini =  0.291424139673\n",
      "[0]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.258048\n",
      "Multiple eval metrics have been passed: 'validation_0-gini' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-gini hasn't improved in 25 rounds.\n",
      "[20]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.274712\n",
      "[40]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.279174\n",
      "[60]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.279428\n",
      "[80]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.283183\n",
      "[100]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.283083\n",
      "[120]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.283911\n",
      "[140]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.284398\n",
      "[160]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.284237\n",
      "Stopping. Best iteration:\n",
      "[152]\tvalidation_0-error:0.036441\tvalidation_0-gini:-0.284716\n",
      "\n",
      "Val Gini =  0.284273104191\n",
      "0.288807037429\n"
     ]
    }
   ],
   "source": [
    "# use xgboost as meta model\n",
    "xgb1_params = {}\n",
    "xgb1_params['objective'] = \"binary:logistic\"\n",
    "xgb1_params['n_estimators'] = 400 # MAX_ROUNDS\n",
    "xgb1_params['learning_rate'] = 0.01\n",
    "xgb1_params['max_depth'] = 5\n",
    "xgb1_params['subsample'] = 0.9\n",
    "xgb1_params['colsample_bytree'] = 0.9\n",
    "xgb1_params['min_child_weight'] = 6\n",
    "xgb1_params['gamma'] = 8\n",
    "xgb1_params['reg_alpha'] = 8\n",
    "xgb1_params['reg_lambda'] = 1.5\n",
    "xgb1_params['scale_pos_weight'] = 1.6\n",
    "#xgb1_params['max_delta_step'] = 0\n",
    "xgb1_params['seed'] = models_seed\n",
    "stacking_model = XGBClassifier(**xgb1_params)\n",
    "\n",
    "# K-fold CV for hyperparemeter tuning of the stacked model\n",
    "val_gini_stack = []\n",
    "#for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X,y)):\n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = train_meta.loc[train_index, models_use + extra_features].copy(), train_meta.loc[test_index, models_use + extra_features].copy()\n",
    "    eval_set=[(X_valid,y_valid)]\n",
    "    stacking_model = stacking_model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric=gini_xgb,\n",
    "                               early_stopping_rounds=25,\n",
    "                               verbose=20\n",
    "                             )\n",
    "    pred = stacking_model.predict_proba(X_valid)[:,1]\n",
    "    val_gini_stack.append(eval_gini(y_valid, pred))\n",
    "    print(\"Val Gini = \", str(val_gini_stack[-1]))\n",
    "print(np.mean(val_gini_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Gini =  0.288408536878\n",
      "Val Gini =  0.281605198447\n",
      "Val Gini =  0.28326733409\n",
      "Val Gini =  0.288655664293\n",
      "Val Gini =  0.280971229777\n",
      "0.284581592697\n"
     ]
    }
   ],
   "source": [
    "# K-fold CV for hyperparemeter tuning of the stacked model (LR)\n",
    "extra_features = []\n",
    "stacking_model = LogisticRegression(C=100.5)\n",
    "val_gini_stack = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X,y)):\n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = train_meta.loc[train_index, models_use + extra_features].copy(), train_meta.loc[test_index, models_use + extra_features].copy()\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "    pred = stacking_model.predict_proba(X_valid)[:,1]\n",
    "    val_gini_stack.append(eval_gini(y_valid, pred))\n",
    "    print(\"Val Gini = \", str(val_gini_stack[-1]))\n",
    "print(np.mean(val_gini_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with XGBoost\n",
    "# Fit a new model, S (i.e the stacking model) to train_meta, using predictions of other models as features. \n",
    "# Optionally, include other features from the original training dataset or engineered features\n",
    "xgb1_params['n_estimators'] = 130 # MAX_ROUNDS\n",
    "stacking_model = XGBClassifier(**xgb1_params)\n",
    "stacking_model.fit(train_meta[models_use + extra_features], y)\n",
    "# Use the stacked model S to make final predictions on test_meta\n",
    "res = stacking_model.predict_proba(test_meta[models_use + extra_features])[:,1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_meta.id\n",
    "sub['target'] = res\n",
    "sub.to_csv('stacked_2_xgbstacked_stratified.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With LR\n",
    "# Fit a new model, S (i.e the stacking model) to train_meta, using predictions of other models as features. \n",
    "# Optionally, include other features from the original training dataset or engineered features\n",
    "stacking_model = LogisticRegression()\n",
    "stacking_model.fit(train_meta[models_names], y)\n",
    "# Use the stacked model S to make final predictions on test_meta\n",
    "res = stacking_model.predict_proba(test_meta[models_names])[:,1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_meta.id\n",
    "sub['target'] = res\n",
    "sub.to_csv('stacked_2_xgbstacked_stratified.csv', index=False)\n",
    "print(stacking_model.coef_)\n",
    "print(stacking_model.intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show correlation between the different model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e6f11d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGfCAYAAABiCLkcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkiRJbfwTvPCS4ihSvogAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e6c88d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "corr = train_meta[models_names].corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try weighting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb1</th>\n",
       "      <th>xgb2</th>\n",
       "      <th>lgb1</th>\n",
       "      <th>lgb2</th>\n",
       "      <th>catboost1</th>\n",
       "      <th>catboost2</th>\n",
       "      <th>catboost3</th>\n",
       "      <th>RForest1</th>\n",
       "      <th>RForest2</th>\n",
       "      <th>RForest3</th>\n",
       "      <th>ExtraTree1</th>\n",
       "      <th>LR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049214</td>\n",
       "      <td>0.038178</td>\n",
       "      <td>0.034175</td>\n",
       "      <td>0.037735</td>\n",
       "      <td>0.038324</td>\n",
       "      <td>0.037233</td>\n",
       "      <td>0.035312</td>\n",
       "      <td>0.039965</td>\n",
       "      <td>0.039412</td>\n",
       "      <td>0.042134</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.511523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044588</td>\n",
       "      <td>0.029538</td>\n",
       "      <td>0.039044</td>\n",
       "      <td>0.031475</td>\n",
       "      <td>0.033275</td>\n",
       "      <td>0.031951</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.028709</td>\n",
       "      <td>0.031707</td>\n",
       "      <td>0.030566</td>\n",
       "      <td>0.033507</td>\n",
       "      <td>0.500043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.023046</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.018830</td>\n",
       "      <td>0.019173</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.019945</td>\n",
       "      <td>0.023162</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>0.029169</td>\n",
       "      <td>0.406342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021312</td>\n",
       "      <td>0.014280</td>\n",
       "      <td>0.013969</td>\n",
       "      <td>0.015773</td>\n",
       "      <td>0.014852</td>\n",
       "      <td>0.016024</td>\n",
       "      <td>0.015207</td>\n",
       "      <td>0.016184</td>\n",
       "      <td>0.016653</td>\n",
       "      <td>0.015075</td>\n",
       "      <td>0.024602</td>\n",
       "      <td>0.306179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062117</td>\n",
       "      <td>0.035459</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>0.033303</td>\n",
       "      <td>0.028968</td>\n",
       "      <td>0.035890</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>0.041794</td>\n",
       "      <td>0.043107</td>\n",
       "      <td>0.034419</td>\n",
       "      <td>0.511595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       xgb1      xgb2      lgb1      lgb2  catboost1  catboost2  catboost3  \\\n",
       "0  0.049214  0.038178  0.034175  0.037735   0.038324   0.037233   0.035312   \n",
       "1  0.044588  0.029538  0.039044  0.031475   0.033275   0.031951   0.030196   \n",
       "2  0.034072  0.023046  0.014451  0.018830   0.019173   0.020481   0.019945   \n",
       "3  0.021312  0.014280  0.013969  0.015773   0.014852   0.016024   0.015207   \n",
       "4  0.062117  0.035459  0.029056  0.033303   0.028968   0.035890   0.036607   \n",
       "\n",
       "   RForest1  RForest2  RForest3  ExtraTree1       LR1  \n",
       "0  0.039965  0.039412  0.042134    0.051316  0.511523  \n",
       "1  0.028709  0.031707  0.030566    0.033507  0.500043  \n",
       "2  0.023162  0.022777  0.026969    0.029169  0.406342  \n",
       "3  0.016184  0.016653  0.015075    0.024602  0.306179  \n",
       "4  0.044305  0.041794  0.043107    0.034419  0.511595  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta[models_names].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.288781954259\n",
      "1.0\n",
      "-------------\n",
      "0.288893072662\n",
      "1.0\n",
      "-------------\n",
      "0.288515743622\n",
      "0.99\n",
      "-------------\n",
      "0.288769470153\n",
      "1.0000000000000002\n",
      "-------------\n",
      "0.288398039086\n",
      "1.0000000000000002\n",
      "-------------\n",
      "0.288853786057\n",
      "1.0\n",
      "-------------\n",
      "0.285686092729\n",
      "1.0\n",
      "-------------\n",
      "0.289231596214\n",
      "1.0\n",
      "-------------\n",
      "0.289171385954\n",
      "1.0\n",
      "-------------\n",
      "0.289239984643\n",
      "1.0000000000000002\n",
      "-------------\n",
      "0.289372613267\n",
      "1.0\n",
      "-------------\n",
      "0.289206873943\n",
      "0.9810000000000001\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "#xgb1: 0.288046013899\n",
    "#xgb2: 0.284421805069\n",
    "#lgb1: 0.28742875413\n",
    "#lgb2: 0.287559384506\n",
    "#catboost1: 0.28591085648\n",
    "#catboost2: 0.280604361918\n",
    "#catboost3: 0.285791647191\n",
    "#RForest1: 0.274770010978\n",
    "#RForest2: 0.276164442753\n",
    "#RForest3: 0.27634030336\n",
    "#ExtraTree1: 0.255998723511\n",
    "#LR1: 0.270042410499\n",
    "l_weights = []\n",
    "l_weights.append({'xgb1': 0.32, 'xgb2': 0.09, 'lgb1': 0.17, 'lgb2': 0.17, 'catboost1': 0.07, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.07, 'RForest1': 0.01, 'RForest2': 0.02, 'RForest3': 0.02, 'ExtraTree1': 0.01, 'LR1': 0.02 })\n",
    "l_weights.append({'xgb1': 0.35, 'xgb2': 0.06, 'lgb1': 0.20, 'lgb2': 0.20, 'catboost1': 0.05, 'catboost2': 0.02, \n",
    "                  'catboost3': 0.05, 'RForest1': 0.01, 'RForest2': 0.01, 'RForest3': 0.02, 'ExtraTree1': 0.01, 'LR1': 0.02 })\n",
    "l_weights.append({'xgb1': 0.25, 'xgb2': 0.05, 'lgb1': 0.20, 'lgb2': 0.20, 'catboost1': 0.07, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.07, 'RForest1': 0.01, 'RForest2': 0.02, 'RForest3': 0.04, 'ExtraTree1': 0.02, 'LR1': 0.03 })\n",
    "l_weights.append({'xgb1': 0.26, 'xgb2': 0.07, 'lgb1': 0.15, 'lgb2': 0.15, 'catboost1': 0.12, 'catboost2': 0.05, \n",
    "                  'catboost3': 0.12, 'RForest1': 0.01, 'RForest2': 0.02, 'RForest3': 0.02, 'ExtraTree1': 0.01, 'LR1': 0.02 })\n",
    "l_weights.append({'xgb1': 0.26, 'xgb2': 0.07, 'lgb1': 0.16, 'lgb2': 0.16, 'catboost1': 0.08, 'catboost2': 0.04, \n",
    "                  'catboost3': 0.08, 'RForest1': 0.01, 'RForest2': 0.03, 'RForest3': 0.07, 'ExtraTree1': 0.01, 'LR1': 0.03 })\n",
    "l_weights.append({'xgb1': 0.5, 'xgb2': 0.05, 'lgb1': 0.12, 'lgb2': 0.12, 'catboost1': 0.06, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.06, 'RForest1': 0.01, 'RForest2': 0.01, 'RForest3': 0.02, 'ExtraTree1': 0.01, 'LR1': 0.01 })\n",
    "l_weights.append({'xgb1': 0.25, 'xgb2': 0.05, 'lgb1': 0.20, 'lgb2': 0.20, 'catboost1': 0.06, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.06, 'RForest1': 0.01, 'RForest2': 0.01, 'RForest3': 0.02, 'ExtraTree1': 0.01, 'LR1': 0.10 })\n",
    "l_weights.append({'xgb1': 0.15, 'xgb2': 0.05, 'lgb1': 0.3, 'lgb2': 0.3, 'catboost1': 0.06, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.06, 'RForest1': 0.01, 'RForest2': 0.01, 'RForest3': 0.01, 'ExtraTree1': 0.01, 'LR1': 0.01 })\n",
    "l_weights.append({'xgb1': 0.325, 'xgb2': 0.09, 'lgb1': 0.17, 'lgb2': 0.17, 'catboost1': 0.07, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.08, 'RForest1': 0.01, 'RForest2': 0.02, 'RForest3': 0.02, 'ExtraTree1': 0.01, 'LR1': 0.005 })\n",
    "l_weights.append({'xgb1': 0.16, 'xgb2': 0.04, 'lgb1': 0.31, 'lgb2': 0.31, 'catboost1': 0.05, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.05, 'RForest1': 0.01, 'RForest2': 0.01, 'RForest3': 0.01, 'ExtraTree1': 0.01, 'LR1': 0.01 })\n",
    "l_weights.append({'xgb1': 0.31, 'xgb2': 0.02, 'lgb1': 0.25, 'lgb2': 0.25, 'catboost1': 0.05, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.05, 'RForest1': 0.005, 'RForest2': 0.01, 'RForest3': 0.01, 'ExtraTree1': 0.01, 'LR1': 0.005 })\n",
    "l_weights.append({'xgb1': 0.10, 'xgb2': 0.01, 'lgb1': 0.40, 'lgb2': 0.40, 'catboost1': 0.015, 'catboost2': 0.01, \n",
    "                  'catboost3': 0.01, 'RForest1': 0.001, 'RForest2': 0.01, 'RForest3': 0.01, 'ExtraTree1': 0.01, 'LR1': 0.005 })\n",
    "    \n",
    "for weights in l_weights:\n",
    "    sum = 0\n",
    "    train_meta['weighted_pred'] = 0\n",
    "    test_meta['weighted_pred'] = 0\n",
    "    for w in weights:\n",
    "        sum += weights[w]\n",
    "        train_meta['weighted_pred'] += train_meta[w] * weights[w]\n",
    "        test_meta['weighted_pred'] += test_meta[w]  * weights[w]\n",
    "    print(eval_gini(y, train_meta['weighted_pred']))\n",
    "    print(sum)\n",
    "    print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.290012923856\n",
      "1.0\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "weights = {'xgb1': 0.31, 'xgb2': 0.02, 'lgb1': 0.25, 'lgb2': 0.25, 'catboost1': 0.05, 'catboost2': 0.03, \n",
    "                  'catboost3': 0.05, 'RForest1': 0.005, 'RForest2': 0.01, 'RForest3': 0.01, 'ExtraTree1': 0.01, 'LR1': 0.005 }\n",
    "\n",
    "train_meta['weighted_pred'] = 0\n",
    "test_meta['weighted_pred'] = 0\n",
    "sum = 0\n",
    "for w in weights:\n",
    "    sum += weights[w]\n",
    "    train_meta['weighted_pred'] += train_meta[w] * weights[w]\n",
    "    test_meta['weighted_pred'] += test_meta[w]  * weights[w]\n",
    "print(eval_gini(y, train_meta['weighted_pred']))\n",
    "print(sum)\n",
    "print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_meta.id\n",
    "sub['target'] = test_meta['weighted_pred']\n",
    "sub.to_csv('stacked_2_weighted_stratified.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using log-odds and mean ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load from some public kernels\n",
    "df_public_kernel_valid = pd.read_csv('public_kernels/NN_EntityEmbed_10fold-val_preds.csv')\n",
    "df_public_kernel_test = pd.read_csv('public_kernels/NN_EntityEmbed_10fold-sub.csv')\n",
    "train_meta['NN_EntityEmbed'] = df_public_kernel_valid['0']\n",
    "test_meta['NN_EntityEmbed'] = df_public_kernel_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply log-odds transformation, regularize, and take standardized mean\n",
    "logits = concat_df.applymap(lambda x: np.log(x/(1-x)))\n",
    "logits *= (1 - ALPHA*logits**2)\n",
    "stdevs = logits.std()  \n",
    "w = .2/stdevs\n",
    "wa = (w*logits).sum(axis=1)/w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PROBABLY USE THIS\n",
    "\n",
    "# Apply ranking, normalization and averaging\n",
    "concat_df[\"target\"] = (concat_df.rank() / concat_df.shape[0]).mean(axis=1)\n",
    "concat_df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "# Read and concatenate submissions\n",
    "outs = [pd.read_csv(os.path.join(submissions_path, f), index_col=0)\\\n",
    "        for f in all_files]\n",
    "concat_df = pd.concat(outs, axis=1)\n",
    "cols = list(map(lambda x: \"target_\" + str(x), range(len(concat_df.columns))))\n",
    "concat_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.290323179537\n"
     ]
    }
   ],
   "source": [
    "#models_use = ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'catboost1', 'catboost2', 'catboost3', 'RForest1', 'RForest2', 'RForest3', 'ExtraTree1', 'LR1', 'NN_EntityEmbed']\n",
    "models_use = ['xgb1', 'lgb1', 'lgb2', 'catboost1', 'NN_EntityEmbed']\n",
    "mean_ranks = (train_meta[models_use].rank() / train_meta.shape[0]).mean(axis=1)\n",
    "print((eval_gini(y, mean_ranks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking with mean ranks or log odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_use = ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'catboost1', 'catboost2', 'catboost3', 'RForest1', 'RForest2', 'RForest3', 'ExtraTree1', 'LR1', 'NN_EntityEmbed']\n",
    "models_use = ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'catboost1', 'catboost2', 'catboost3', 'RForest1', 'RForest3', 'LR1', 'NN_EntityEmbed'] #0.290696537205 #0.290631212567 \n",
    "#models_use = ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'catboost1', 'catboost3', 'RForest1', 'RForest3', 'LR1', 'NN_EntityEmbed']              #0.29063358156  #0.290558840017\n",
    "# Compute ranks for each model\n",
    "rank_columns = []\n",
    "for m in models_use:\n",
    "    train_meta[m + '_rank'] = train_meta[m].rank() / train_meta.shape[0]\n",
    "    test_meta[m + '_rank'] = test_meta[m].rank() / test_meta.shape[0]\n",
    "    rank_columns.append(m + '_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.56694394 -0.26109092  0.54588509  0.04930208  0.36488222 -0.35267553\n",
      "   0.39455724 -0.22943743  0.36151151 -0.03081672  0.46452053]]\n",
      "[-4.33845315]\n",
      "Val Gini =  0.295728061086\n",
      "[[ 0.54692849 -0.33557754  0.52991178  0.07307778  0.36509989 -0.30431096\n",
      "   0.31107822 -0.22470054  0.39014001  0.00601314  0.53418532]]\n",
      "[-4.3485683]\n",
      "Val Gini =  0.28915771411\n",
      "[[ 0.79958212 -0.48410973  0.51266035 -0.02170763  0.26468171 -0.35220213\n",
      "   0.50596225 -0.1802738   0.42069866 -0.03175609  0.45414021]]\n",
      "[-4.34701222]\n",
      "Val Gini =  0.28877589424\n",
      "[[  7.10696144e-01  -3.34657260e-01   5.05342018e-01   1.40245016e-01\n",
      "    4.52141952e-01  -5.45635393e-01   4.03408566e-01  -1.62831983e-01\n",
      "    2.95882217e-01   6.77923095e-04   4.06772628e-01]]\n",
      "[-4.33807014]\n",
      "Val Gini =  0.294993175527\n",
      "[[ 0.63498034 -0.53480705  0.5882816   0.27645017  0.38423141 -0.39267648\n",
      "   0.24679846 -0.16444583  0.37341903 -0.01783521  0.50225036]]\n",
      "[-4.35212088]\n",
      "Val Gini =  0.284838462701\n",
      "0.290698661533\n",
      "Val Gini =  0.290632940117\n"
     ]
    }
   ],
   "source": [
    "# K-fold CV for hyperparemeter tuning of the stacked model (LR)\n",
    "y_valid_pred = 0*y\n",
    "extra_features = []\n",
    "stacking_model = LogisticRegression(C=5, fit_intercept=True)\n",
    "val_gini_stack = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X,y)):\n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = train_meta.loc[train_index, rank_columns].copy(), train_meta.loc[test_index, rank_columns].copy()\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "    print(stacking_model.coef_)\n",
    "    print(stacking_model.intercept_ )\n",
    "    pred = stacking_model.predict_proba(X_valid)[:,1]\n",
    "    val_gini_stack.append(eval_gini(y_valid, pred))\n",
    "    print(\"Val Gini = \", str(val_gini_stack[-1]))\n",
    "    y_valid_pred.iloc[test_index] = pred \n",
    "print(np.mean(val_gini_stack))\n",
    "print(\"Val Gini = \", str(eval_gini(y, y_valid_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.65217836 -0.39287841  0.53619447  0.10421158  0.36641749 -0.39063114\n",
      "   0.37254124 -0.19116798  0.36863654 -0.01384492  0.47278352]]\n",
      "[-4.34485148]\n"
     ]
    }
   ],
   "source": [
    "# With LR\n",
    "# Fit a new model, S (i.e the stacking model) to train_meta, using predictions of other models as features. \n",
    "# Optionally, include other features from the original training dataset or engineered features\n",
    "stacking_model = LogisticRegression(C=5)\n",
    "stacking_model.fit(train_meta[rank_columns], y)\n",
    "# Use the stacked model S to make final predictions on test_meta\n",
    "res = stacking_model.predict_proba(test_meta[rank_columns])[:,1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_meta.id\n",
    "sub['target'] = res\n",
    "sub.to_csv('stacked_2_lr_stratified.csv', index=False)\n",
    "print(stacking_model.coef_)\n",
    "print(stacking_model.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>xgb1</th>\n",
       "      <th>xgb2</th>\n",
       "      <th>lgb1</th>\n",
       "      <th>lgb2</th>\n",
       "      <th>catboost1</th>\n",
       "      <th>catboost2</th>\n",
       "      <th>catboost3</th>\n",
       "      <th>RForest1</th>\n",
       "      <th>RForest2</th>\n",
       "      <th>...</th>\n",
       "      <th>xgb2_rank</th>\n",
       "      <th>lgb1_rank</th>\n",
       "      <th>lgb2_rank</th>\n",
       "      <th>catboost1_rank</th>\n",
       "      <th>catboost2_rank</th>\n",
       "      <th>catboost3_rank</th>\n",
       "      <th>RForest1_rank</th>\n",
       "      <th>RForest3_rank</th>\n",
       "      <th>LR1_rank</th>\n",
       "      <th>NN_EntityEmbed_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.025420</td>\n",
       "      <td>0.025559</td>\n",
       "      <td>0.023955</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.024433</td>\n",
       "      <td>0.023149</td>\n",
       "      <td>0.023099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251206</td>\n",
       "      <td>0.322029</td>\n",
       "      <td>0.317567</td>\n",
       "      <td>0.266391</td>\n",
       "      <td>0.233828</td>\n",
       "      <td>0.284036</td>\n",
       "      <td>0.160053</td>\n",
       "      <td>0.219015</td>\n",
       "      <td>0.281517</td>\n",
       "      <td>0.475982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.023530</td>\n",
       "      <td>0.023353</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>0.023760</td>\n",
       "      <td>0.023367</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>0.025149</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239380</td>\n",
       "      <td>0.258602</td>\n",
       "      <td>0.225554</td>\n",
       "      <td>0.260518</td>\n",
       "      <td>0.225471</td>\n",
       "      <td>0.210697</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.176814</td>\n",
       "      <td>0.250817</td>\n",
       "      <td>0.279407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.034893</td>\n",
       "      <td>0.024356</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>0.022635</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.024145</td>\n",
       "      <td>0.023330</td>\n",
       "      <td>0.027521</td>\n",
       "      <td>0.026315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265768</td>\n",
       "      <td>0.204728</td>\n",
       "      <td>0.228670</td>\n",
       "      <td>0.210119</td>\n",
       "      <td>0.251002</td>\n",
       "      <td>0.249840</td>\n",
       "      <td>0.290317</td>\n",
       "      <td>0.210399</td>\n",
       "      <td>0.224122</td>\n",
       "      <td>0.246697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.022260</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>0.014309</td>\n",
       "      <td>0.015621</td>\n",
       "      <td>0.014096</td>\n",
       "      <td>0.017336</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029387</td>\n",
       "      <td>0.068201</td>\n",
       "      <td>0.031107</td>\n",
       "      <td>0.034473</td>\n",
       "      <td>0.031708</td>\n",
       "      <td>0.030565</td>\n",
       "      <td>0.041165</td>\n",
       "      <td>0.034994</td>\n",
       "      <td>0.092570</td>\n",
       "      <td>0.056965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.062924</td>\n",
       "      <td>0.040745</td>\n",
       "      <td>0.039816</td>\n",
       "      <td>0.038227</td>\n",
       "      <td>0.038177</td>\n",
       "      <td>0.037614</td>\n",
       "      <td>0.037237</td>\n",
       "      <td>0.043257</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705585</td>\n",
       "      <td>0.684544</td>\n",
       "      <td>0.655449</td>\n",
       "      <td>0.649888</td>\n",
       "      <td>0.642314</td>\n",
       "      <td>0.635101</td>\n",
       "      <td>0.732599</td>\n",
       "      <td>0.728031</td>\n",
       "      <td>0.659231</td>\n",
       "      <td>0.558137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      xgb1      xgb2      lgb1      lgb2  catboost1  catboost2  \\\n",
       "0   0  0.038611  0.023903  0.025420  0.025559   0.023955   0.023623   \n",
       "1   1  0.032867  0.023530  0.023353  0.022532   0.023760   0.023367   \n",
       "2   2  0.034893  0.024356  0.021546  0.022635   0.022074   0.024145   \n",
       "3   3  0.022260  0.014952  0.016135  0.014097   0.014309   0.015621   \n",
       "4   4  0.062924  0.040745  0.039816  0.038227   0.038177   0.037614   \n",
       "\n",
       "   catboost3  RForest1  RForest2         ...           xgb2_rank  lgb1_rank  \\\n",
       "0   0.024433  0.023149  0.023099         ...            0.251206   0.322029   \n",
       "1   0.022031  0.025149  0.024097         ...            0.239380   0.258602   \n",
       "2   0.023330  0.027521  0.026315         ...            0.265768   0.204728   \n",
       "3   0.014096  0.017336  0.017242         ...            0.029387   0.068201   \n",
       "4   0.037237  0.043257  0.043151         ...            0.705585   0.684544   \n",
       "\n",
       "   lgb2_rank  catboost1_rank  catboost2_rank  catboost3_rank  RForest1_rank  \\\n",
       "0   0.317567        0.266391        0.233828        0.284036       0.160053   \n",
       "1   0.225554        0.260518        0.225471        0.210697       0.216500   \n",
       "2   0.228670        0.210119        0.251002        0.249840       0.290317   \n",
       "3   0.031107        0.034473        0.031708        0.030565       0.041165   \n",
       "4   0.655449        0.649888        0.642314        0.635101       0.732599   \n",
       "\n",
       "   RForest3_rank  LR1_rank  NN_EntityEmbed_rank  \n",
       "0       0.219015  0.281517             0.475982  \n",
       "1       0.176814  0.250817             0.279407  \n",
       "2       0.210399  0.224122             0.246697  \n",
       "3       0.034994  0.092570             0.056965  \n",
       "4       0.728031  0.659231             0.558137  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
