{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gini metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "    \n",
    "    \n",
    "# Funcitons from olivier's kernel\n",
    "# https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv('data/train.csv', na_values=\"-1\") # .iloc[0:200,:]\n",
    "#test_df = pd.read_csv('data/test.csv', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in train_df.columns:\n",
    "    if train_df[c].dtype == np.float64:\n",
    "        train_df[c] = train_df[c].astype(np.float32)\n",
    "        #test_df[c] = test_df[c].astype(np.float32)\n",
    "    elif train_df[c].dtype == np.int64:\n",
    "        train_df[c] = train_df[c].astype(np.int32)\n",
    "        #if c in test_df:\n",
    "            #test_df[c] = test_df[c].astype(np.int32)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from olivier\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "    #\"nulls_count\" #            : not analized\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "numerical_features = ['ps_ind_01', \n",
    "                     'ps_ind_03',\n",
    "                     'ps_ind_14',\n",
    "                     'ps_ind_15',\n",
    "                     'ps_reg_01',\n",
    "                     'ps_reg_02',\n",
    "                     'ps_reg_03',\n",
    "                     'ps_car_11',\n",
    "                     'ps_car_12',\n",
    "                     'ps_car_13',\n",
    "                     'ps_car_14',\n",
    "                     'ps_car_15',\n",
    "                     'ps_calc_01',\n",
    "                     'ps_calc_02',\n",
    "                     'ps_calc_03',\n",
    "                     'ps_calc_04',\n",
    "                     'ps_calc_05',\n",
    "                     'ps_calc_06',\n",
    "                     'ps_calc_07',\n",
    "                     'ps_calc_08',\n",
    "                     'ps_calc_09',\n",
    "                     'ps_calc_10',\n",
    "                     'ps_calc_11',\n",
    "                     'ps_calc_12',\n",
    "                     'ps_calc_13',\n",
    "                     'ps_calc_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ind_01\n",
      "ps_ind_03\n",
      "ps_ind_14\n",
      "ps_ind_15\n",
      "ps_reg_01\n",
      "ps_reg_02\n",
      "ps_reg_03\n",
      "ps_car_11\n",
      "ps_car_12\n",
      "ps_car_13\n",
      "ps_car_14\n",
      "ps_car_15\n",
      "ps_calc_01\n",
      "ps_calc_02\n",
      "ps_calc_03\n",
      "ps_calc_04\n",
      "ps_calc_05\n",
      "ps_calc_06\n",
      "ps_calc_07\n",
      "ps_calc_08\n",
      "ps_calc_09\n",
      "ps_calc_10\n",
      "ps_calc_11\n",
      "ps_calc_12\n",
      "ps_calc_13\n",
      "ps_calc_14\n"
     ]
    }
   ],
   "source": [
    "for f1 in numerical_features:\n",
    "    print(f1)\n",
    "    for f2 in numerical_features: \n",
    "        if f1 != f2:\n",
    "            train_df['sum_' + f1 + '_' + f2] = (train_df[f1] + train_df[f2]).astype(np.float32)\n",
    "            #test_df['sum_' + f1 + '_' + f2] = (test_df[f1] + test_df[f2]).astype(np.float32)\n",
    "            #train_df['diff_' + f1 + '_' + f2] = train_df[f1] - train_df[f2]\n",
    "            #test_df['diff_' + f1 + '_' + f2] = test_df[f1] - test_df[f2]\n",
    "            #train_df['mult_' + f1 + '_' + f2] = train_df[f1] * train_df[f2]\n",
    "            #test_df['mult_' + f1 + '_' + f2] = test_df[f1] * test_df[f2]\n",
    "            #train_df['div_' + f1 + '_' + f2] = train_df[f1] / train_df[f2]\n",
    "            #test_df['div_' + f1 + '_' + f2] = test_df[f1] / test_df[f2]\n",
    "            train_features.append('sum_' + f1 + '_' + f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data\n",
    "#id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "    \n",
    "X = train_df[train_features]\n",
    "#test_df = test_df[train_features]\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb1_params = {}\n",
    "xgb1_params['objective'] = \"binary:logistic\"\n",
    "xgb1_params['n_estimators'] = 400 # MAX_ROUNDS\n",
    "xgb1_params['learning_rate'] = 0.07\n",
    "xgb1_params['max_depth'] = 4\n",
    "xgb1_params['subsample'] = 0.80\n",
    "xgb1_params['colsample_bytree'] = 0.80\n",
    "xgb1_params['min_child_weight'] = 6\n",
    "xgb1_params['gamma'] = 10\n",
    "xgb1_params['reg_alpha'] = 8\n",
    "xgb1_params['reg_lambda'] = 1.5\n",
    "xgb1_params['scale_pos_weight'] = 1.6\n",
    "#xgb1_params['max_delta_step'] = 0\n",
    "xgb1_params['seed'] = 0\n",
    "xgb1 = XGBClassifier(**xgb1_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = 0*y\n",
    "#y_test_pred = 0\n",
    "\n",
    "\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 0, shuffle = True)\n",
    "# Also try with stratified\n",
    "np.random.seed(0)\n",
    "\n",
    "# Run CV\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    #X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "       \n",
    "    #print('Parameters: ')\n",
    "    #print(model_params)\n",
    "    fit_model = xgb1.fit(X_train, y_train)\n",
    "    # if xgboost model, save it\n",
    "    #if 'xgb' in model_name:\n",
    "    fit_model.booster().dump_model(model_name + '_fold' + str(i) + '.dump',  with_stats=True)\n",
    "\n",
    "    # Train error\n",
    "    pred = fit_model.predict_proba(X_train)[:,1]\n",
    "    print(\"TrainGini = \", str(eval_gini(y_train, pred)))\n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    l_gini_val.append(eval_gini(y_valid, pred))\n",
    "    print(\"Val Gini = \", str(eval_gini(y_valid, pred)))\n",
    "    y_valid_pred.iloc[test_index] = pred \n",
    "\n",
    "    # Accumulate test set predictions\n",
    "    #y_test_pred += fit_model.predict_proba(X_test)[:,1] \n",
    "\n",
    "    # Plot importance plot\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    plot_importance(booster=booster, ax=ax, **kwargs)\n",
    "    fig.savefig('xgb_' + str(i))\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "          \n",
    "          \n",
    "        \n",
    "    print('---------------------------------------END FOLD------------------------------------------')\n",
    "    \n",
    "#y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "print('END KFOLD')\n",
    "print( \"\\nGini for full training set:\" )\n",
    "print(eval_gini(y, y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
